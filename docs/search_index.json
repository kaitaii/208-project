[["index.html", "CMSC/LING/STAT 208: Final Project Overview", " CMSC/LING/STAT 208: Final Project Yi Lu, Shiyi Yang March 10, 2023 02:22 UTC Overview This overview provides a brief explanation of this book. This book has two main sections, DataCamp and Econometrics. Each is described below. The DataCamp section is notes on DataCamp courses. Much of the content in these chapters is directly copy/pasted from DataCamp. Typically this is plagiarism, something important to avoid both at and outside of Lawrence. However, directly copy/pasted content is an important aspect of this BP and it isn’t feasible to put quotation marks around all of it. So, we will give credit where credit is due here. Let this sentence serve as attribution and citation: except for the portions of the DataCamp section that are my own notes supplementing the DataCamp content, which are indicated in red font using the “mynotes” html tag, all content comes directly from DataCamp and the author of this book is giving them credit for their content. Note that the first two chapters are included as examples to give you an idea of what to do. The Econometric section has chapters based on econometrics content covered in class. Content and data in those sections come from the sources discussed in those chapters. These chapters serve as a reference for important concepts and technical skills, as well as examples of how to implement them in R. The first file in bookdown is always index.rmd (and the resulting output file, index.html, is the first file GitHub pages will display). Thus, this must be the first file of your book. This file has this overview and the General Notes section that follows (and initially there are brief instructions, but those are deleted once they are no longer needed). "],["ames-housing.html", "1 Ames Housing", " 1 Ames Housing library(tidyverse) ## ── Attaching core tidyverse packages ─────────── ## ✔ dplyr 1.1.0 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.1 ✔ tibble 3.1.8 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the ]8;;http://conflicted.r-lib.org/conflicted package]8;; to force all conflicts to become errors library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## ## The following object is masked from &#39;package:purrr&#39;: ## ## lift library(recipes) ## ## Attaching package: &#39;recipes&#39; ## ## The following object is masked from &#39;package:stringr&#39;: ## ## fixed ## ## The following object is masked from &#39;package:stats&#39;: ## ## step library(mgcv) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## ## This is mgcv 1.8-41. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. ames&lt;-readRDS(&quot;AmesHousing.rds&quot;) glimpse(ames) # Knowing that for this dataset n = 881, p = 20-1 = 19. ## Rows: 881 ## Columns: 20 ## $ Sale_Price &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, … ## $ Gr_Liv_Area &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, … ## $ Garage_Cars &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,… ## $ Garage_Area &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Neighborhood &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest… ## $ Screen_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Overall_Qual &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average… ## $ Lot_Area &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101… ## $ Lot_Frontage &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6… ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_… ## $ Misc_Val &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,… ## $ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7… ## $ First_Flr_SF &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11… ## $ Year_Built &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20… sum(is.na(ames)) # check for missing entries ## [1] 113 summary(ames) # check types of features, which features have missing entries? ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 34900 Min. : 334 Attchd :514 Min. :0.000 ## 1st Qu.:129500 1st Qu.:1118 Basment : 10 1st Qu.:1.000 ## Median :160000 Median :1442 BuiltIn : 55 Median :2.000 ## Mean :181115 Mean :1495 CarPort : 5 Mean :1.762 ## 3rd Qu.:213500 3rd Qu.:1728 Detchd :234 3rd Qu.:2.000 ## Max. :755000 Max. :5642 More_Than_Two_Types: 9 Max. :4.000 ## NA&#39;s :113 No_Garage : 54 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:880 Min. : 0.00 ## 1st Qu.: 324.0 Pave:877 NoSeWa: 0 1st Qu.: 0.00 ## Median : 480.0 NoSewr: 1 Median : 0.00 ## Mean : 476.5 Mean : 2.41 ## 3rd Qu.: 592.0 3rd Qu.: 0.00 ## Max. :1418.0 Max. :576.00 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames :127 Min. : 0.00 Average :243 Min. : 1300 ## College_Creek : 86 1st Qu.: 0.00 Above_Average:217 1st Qu.: 7449 ## Old_Town : 83 Median : 0.00 Good :177 Median : 9512 ## Northridge_Heights: 52 Mean : 18.11 Very_Good : 99 Mean : 10105 ## Somerset : 50 3rd Qu.: 0.00 Below_Average: 83 3rd Qu.: 11526 ## Edwards : 49 Max. :490.00 Excellent : 38 Max. :159000 ## (Other) :434 (Other) : 24 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.00 One_Story_1946_and_Newer_All_Styles :335 Min. : 0.00 ## 1st Qu.: 43.00 Two_Story_1946_and_Newer :171 1st Qu.: 0.00 ## Median : 63.00 One_and_Half_Story_Finished_All_Ages: 92 Median : 0.00 ## Mean : 57.78 One_Story_PUD_1946_and_Newer : 53 Mean : 37.97 ## 3rd Qu.: 78.00 Duplex_All_Styles_and_Ages : 40 3rd Qu.: 0.00 ## Max. :313.00 One_Story_1945_and_Older : 36 Max. :8300.00 ## (Other) :154 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 877 1st Qu.: 0.0 ## Median : 27.00 Median : 6.000 Median :1092 Median : 0.0 ## Mean : 49.93 Mean : 6.413 Mean :1171 Mean : 319.6 ## 3rd Qu.: 72.00 3rd Qu.: 7.000 3rd Qu.:1426 3rd Qu.: 682.0 ## Max. :742.00 Max. :12.000 Max. :4692 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2010 ## levels(ames$Overall_Qual) # the levels are NOT properly ordered ## [1] &quot;Above_Average&quot; &quot;Average&quot; &quot;Below_Average&quot; &quot;Excellent&quot; ## [5] &quot;Fair&quot; &quot;Good&quot; &quot;Poor&quot; &quot;Very_Excellent&quot; ## [9] &quot;Very_Good&quot; &quot;Very_Poor&quot; # relevel the levels ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) levels(ames$Overall_Qual) # the levels are properly ordered ## [1] &quot;Very_Poor&quot; &quot;Poor&quot; &quot;Fair&quot; &quot;Below_Average&quot; ## [5] &quot;Average&quot; &quot;Above_Average&quot; &quot;Good&quot; &quot;Very_Good&quot; ## [9] &quot;Excellent&quot; &quot;Very_Excellent&quot; # split the dataset set.seed(013123) # set seed index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # consider 70-30 split ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data # investigate nominal categorical predictors ames_train %&gt;% count(Neighborhood) %&gt;% arrange(n) # check frequency of categories ## # A tibble: 26 × 2 ## Neighborhood n ## &lt;fct&gt; &lt;int&gt; ## 1 Northpark_Villa 2 ## 2 Blueste 2 ## 3 Greens 4 ## 4 Veenker 6 ## 5 Bloomington_Heights 7 ## 6 Briardale 8 ## 7 Clear_Creek 11 ## 8 Meadow_Village 12 ## 9 Stone_Brook 13 ## 10 Timberland 14 ## # … with 16 more rows # finally, after all preprocessing steps have been decided set up the overall blueprint ames_recipe &lt;- recipe(Sale_Price~., data = ames_train) # set up recipe # specify feature engineering steps blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% # filter out zv/nzv predictors step_impute_mean(Gr_Liv_Area) %&gt;% # impute missing entries step_integer(Overall_Qual) %&gt;% # numeric conversion of levels of the predictors step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response) step_scale(all_numeric(), -all_outcomes()) %&gt;% # scale (divide by standard deviation) all numeric predictors step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% # lumping required predictors step_dummy(all_nominal(), one_hot = FALSE) # one-hot/dummy encode nominal categorical predictors # replace step_center and step_scale with step_normalize prepare &lt;- prep(blueprint, data = ames_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = ames_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = ames_test) # apply the blueprint to test data for future use set.seed(111) model&lt;-train(blueprint, data=ames_train, method=&quot;gam&quot;, tuneGrid=data.frame(method = &quot;GCV.Cp&quot;, select = TRUE), trControl=trainControl(method=&quot;cv&quot;,number=5) ) model$results$RMSE ## [1] 71882.39 model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + ## Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + ## Neighborhood_Somerset + Neighborhood_Northridge_Heights + ## Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + ## MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + ## MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + ## Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + ## s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + ## s(First_Flr_SF) + s(Lot_Area) ## ## Estimated degrees of freedom: ## 0.961 0.000 3.936 1.065 1.974 3.105 6.489 ## 8.662 7.680 total = 50.87 ## ## GCV score: 722968462 # obtain predictions and test set RMSE # final_model &lt;- gam(Sale_Price~. ,data=baked_train) final_model_preds&lt;- predict(object = model$finalModel, newdata = baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((final_model_preds - baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 31357.73 "],["since-gam.html", "2 Since GAM", " 2 Since GAM glimpse(boston) # p=13 ## Rows: 506 ## Columns: 14 ## $ CRIM &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ ZN &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ INDUS &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ CHAS &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ NOX &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ RM &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ AGE &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ DIS &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ RAD &lt;dbl&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ TAX &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ PTRATIO &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ B &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ LSTAT &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ MEDV &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… sum(is.na(boston)) # check for missing entries ## [1] 0 summary(boston) # check types of features ## CRIM ZN INDUS CHAS ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## NOX RM AGE DIS ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## RAD TAX PTRATIO B ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## LSTAT MEDV ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00 nearZeroVar(boston) # check for near zero variable ## integer(0) 2.0.1 # split the dataset set.seed(013123) # set seed index &lt;- createDataPartition(y = boston$MEDV, p = 0.8, list = FALSE) # consider 70-30 split boston_train &lt;- boston[index,] # training data boston_test &lt;- boston[-index,] # test data # finally, after all preprocessing steps have been decided set up the overall blueprint boston_recipe &lt;- recipe(MEDV~., data = boston_train) # set up recipe # specify feature engineering steps blueprint &lt;- boston_recipe %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response) step_scale(all_numeric(), -all_outcomes()) # scale (divide by standard deviation) all numeric predictors # replace step_center and step_scale with step_normalize prepare &lt;- prep(blueprint, data = boston_train) # estimate feature engineering parameters based on training data baked_train &lt;- bake(prepare, new_data = boston_train) # apply the blueprint to training data for building final/optimal model baked_test &lt;- bake(prepare, new_data = boston_test) # apply the blueprint to test data for future use tunegrid&lt;-data.frame(method = &quot;GCV.Cp&quot;, select = TRUE) set.seed(111) model&lt;-train(blueprint, data=boston_train, method=&quot;gam&quot;, tuneGrid=data.frame(method = &quot;GCV.Cp&quot;, select = TRUE), #method indicates the smoothing parameter estimation method, GCV for models with unknown scale parameter and Mallows&#39; Cp/UBRE/AIC for models with known scale; select=TRUE adds extra penalty so that the smoothing parameter estimation can completely remove terms from the model trControl=trainControl(method=&quot;cv&quot;,number=5) ) model$results$RMSE ## [1] 3.989444 model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + ## s(NOX) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT) + s(CRIM) ## ## Estimated degrees of freedom: ## 0.000 0.927 2.906 5.636 7.931 4.886 0.000 ## 0.889 8.392 5.813 4.421 total = 44.8 ## ## GCV score: 12.95974 # obtain predictions and test set RMSE final_model &lt;- gam(MEDV~CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + s(NOX) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT) + s(CRIM),data=baked_train) final_model_preds&lt;- predict(object = final_model, newdata = baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((final_model_preds - baked_test$MEDV)^2)) # calculate test set RMSE ## [1] 3.077761 coef(final_model) ## (Intercept) CHAS RAD s(ZN).1 s(ZN).2 ## 2.267715e+01 2.022569e-01 3.655526e+00 5.824985e-08 2.806488e-09 ## s(ZN).3 s(ZN).4 s(ZN).5 s(ZN).6 s(ZN).7 ## -3.620269e-08 -4.589304e-08 2.602031e-08 -2.756246e-08 -1.855087e-08 ## s(ZN).8 s(ZN).9 s(PTRATIO).1 s(PTRATIO).2 s(PTRATIO).3 ## 1.102485e-07 2.693148e-01 9.792057e-09 -1.232843e-08 -4.982888e-09 ## s(PTRATIO).4 s(PTRATIO).5 s(PTRATIO).6 s(PTRATIO).7 s(PTRATIO).8 ## 1.256083e-08 5.046351e-10 1.040306e-08 3.151308e-09 -6.044307e-08 ## s(PTRATIO).9 s(TAX).1 s(TAX).2 s(TAX).3 s(TAX).4 ## -1.633053e+00 3.618700e+00 1.405151e+00 -7.740275e-02 -5.318771e-01 ## s(TAX).5 s(TAX).6 s(TAX).7 s(TAX).8 s(TAX).9 ## -8.093729e-02 -2.683977e-01 -6.878059e-02 -2.982261e+00 -6.225554e+00 ## s(INDUS).1 s(INDUS).2 s(INDUS).3 s(INDUS).4 s(INDUS).5 ## -3.352131e+00 6.087354e-01 2.121811e+00 -4.651431e+00 -4.869233e-01 ## s(INDUS).6 s(INDUS).7 s(INDUS).8 s(INDUS).9 s(NOX).1 ## -4.174749e+00 3.677786e+00 9.805027e+00 -3.332319e+00 -2.779861e+00 ## s(NOX).2 s(NOX).3 s(NOX).4 s(NOX).5 s(NOX).6 ## -1.453622e+01 -7.149468e-01 1.176423e+01 7.867835e+00 -1.345680e+01 ## s(NOX).7 s(NOX).8 s(NOX).9 s(B).1 s(B).2 ## -4.829748e+00 4.389334e+01 5.054269e+00 1.036487e-01 1.103010e+00 ## s(B).3 s(B).4 s(B).5 s(B).6 s(B).7 ## -4.976484e-01 7.795278e-01 2.786051e-01 -7.448779e-01 3.100793e-01 ## s(B).8 s(B).9 s(AGE).1 s(AGE).2 s(AGE).3 ## 2.754377e+00 1.674564e-01 6.130129e-08 3.960807e-08 -1.406218e-09 ## s(AGE).4 s(AGE).5 s(AGE).6 s(AGE).7 s(AGE).8 ## 4.299025e-08 -3.504346e-09 3.671368e-08 -4.430853e-11 1.602136e-07 ## s(AGE).9 s(DIS).1 s(DIS).2 s(DIS).3 s(DIS).4 ## 1.056042e-01 -5.787123e+00 1.296531e+01 -4.342308e+00 -9.456774e+00 ## s(DIS).5 s(DIS).6 s(DIS).7 s(DIS).8 s(DIS).9 ## 4.443850e+00 -8.397223e+00 2.881023e-01 -1.274815e+01 -2.105359e+01 ## s(RM).1 s(RM).2 s(RM).3 s(RM).4 s(RM).5 ## 5.560717e+00 6.823768e+00 4.391045e+00 -3.983784e+00 -2.724092e+00 ## s(RM).6 s(RM).7 s(RM).8 s(RM).9 s(LSTAT).1 ## 2.646801e+00 4.467370e+00 1.992231e+01 -8.990143e+00 1.543097e+00 ## s(LSTAT).2 s(LSTAT).3 s(LSTAT).4 s(LSTAT).5 s(LSTAT).6 ## -6.815432e+00 8.930136e-01 2.659123e+00 1.762691e+00 3.583721e+00 ## s(LSTAT).7 s(LSTAT).8 s(LSTAT).9 s(CRIM).1 s(CRIM).2 ## 3.954289e-01 -1.149471e+01 -7.907894e+00 7.107952e+00 1.420625e+01 ## s(CRIM).3 s(CRIM).4 s(CRIM).5 s(CRIM).6 s(CRIM).7 ## 2.044314e-02 -6.418126e+00 5.476575e+00 6.115682e+00 -1.279049e+00 ## s(CRIM).8 s(CRIM).9 ## 1.908162e+01 -3.100936e+00 summary(final_model) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## MEDV ~ CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + ## s(NOX) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT) + s(CRIM) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.6771 0.1588 142.800 &lt; 2e-16 *** ## CHAS 0.2023 0.1822 1.110 0.267670 ## RAD 3.6555 1.0438 3.502 0.000521 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(ZN) 1.000 1.000 0.572 0.44993 ## s(PTRATIO) 1.000 1.000 30.385 &lt; 2e-16 *** ## s(TAX) 3.082 3.730 8.783 3.23e-06 *** ## s(INDUS) 7.247 8.177 3.245 0.00134 ** ## s(NOX) 9.000 9.000 12.730 &lt; 2e-16 *** ## s(B) 2.089 2.571 1.623 0.17098 ## s(AGE) 1.000 1.000 0.088 0.76629 ## s(DIS) 8.777 8.983 6.870 &lt; 2e-16 *** ## s(RM) 8.566 8.939 21.781 &lt; 2e-16 *** ## s(LSTAT) 6.823 7.926 20.919 &lt; 2e-16 *** ## s(CRIM) 4.548 5.496 8.095 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.882 Deviance explained = 89.8% ## GCV = 11.906 Scale est. = 10.264 n = 407 "]]
