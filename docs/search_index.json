[["ames-housing-dataset-example.html", "2 Ames Housing Dataset Example", " 2 Ames Housing Dataset Example Data Description: First, we use the ames dataset that we used in class, since we want to first learn how to apply this model and then do further investigation. Response variable: 1)Sale_Price: Property sale price in USD Explanatory variable: Gr_Liv_Area: Above grade (ground) living area square feet Garage_Type: Garage location Garage_Cars: Size of garage in car capacity Garage_Area: Size of garage in square feet Street: Type of road access to property Utilities: Type of utilities available Pool_Area: Pool area in square feet Neighborhood: Physical locations within Ames city limits Screen_Porch: Screen porch area in square feet Overall_Qual: Rates the overall material and finish of the house Lot_Area: Lot size in square feet Lot_Frontage: Linear feet of street connected to property MS_SubClass: Identifies the type of dwelling involved in the sale. Misc_Val: Dollar value of miscellaneous feature Open_Porch_SF: Open porch area in square feet TotRms_AbvGrd: Total rooms above grade (does not include bathrooms) · First_Flr_SF: First Floor square feet Second_Flr_SF: Second floor square feet Year_Built: Original construction date "],["preparation-steps.html", "2.1 Preparation Steps", " 2.1 Preparation Steps 2.1.1 Data Investigation Just as before, we first investigate the dataset to see if there’s any missing entries, non-numeric variables, or ZV/NZV features. glimpse(ames) ## Rows: 881 ## Columns: 20 ## $ Sale_Price &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, … ## $ Gr_Liv_Area &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, … ## $ Garage_Cars &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,… ## $ Garage_Area &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Neighborhood &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest… ## $ Screen_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Overall_Qual &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average… ## $ Lot_Area &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101… ## $ Lot_Frontage &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6… ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_… ## $ Misc_Val &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,… ## $ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7… ## $ First_Flr_SF &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11… ## $ Year_Built &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20… sum(is.na(ames)) ## [1] 113 summary(ames) ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 34900 Min. : 334 Attchd :514 Min. :0.000 ## 1st Qu.:129500 1st Qu.:1118 Basment : 10 1st Qu.:1.000 ## Median :160000 Median :1442 BuiltIn : 55 Median :2.000 ## Mean :181115 Mean :1495 CarPort : 5 Mean :1.762 ## 3rd Qu.:213500 3rd Qu.:1728 Detchd :234 3rd Qu.:2.000 ## Max. :755000 Max. :5642 More_Than_Two_Types: 9 Max. :4.000 ## NA&#39;s :113 No_Garage : 54 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:880 Min. : 0.00 ## 1st Qu.: 324.0 Pave:877 NoSeWa: 0 1st Qu.: 0.00 ## Median : 480.0 NoSewr: 1 Median : 0.00 ## Mean : 476.5 Mean : 2.41 ## 3rd Qu.: 592.0 3rd Qu.: 0.00 ## Max. :1418.0 Max. :576.00 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames :127 Min. : 0.00 Average :243 Min. : 1300 ## College_Creek : 86 1st Qu.: 0.00 Above_Average:217 1st Qu.: 7449 ## Old_Town : 83 Median : 0.00 Good :177 Median : 9512 ## Northridge_Heights: 52 Mean : 18.11 Very_Good : 99 Mean : 10105 ## Somerset : 50 3rd Qu.: 0.00 Below_Average: 83 3rd Qu.: 11526 ## Edwards : 49 Max. :490.00 Excellent : 38 Max. :159000 ## (Other) :434 (Other) : 24 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.00 One_Story_1946_and_Newer_All_Styles :335 Min. : 0.00 ## 1st Qu.: 43.00 Two_Story_1946_and_Newer :171 1st Qu.: 0.00 ## Median : 63.00 One_and_Half_Story_Finished_All_Ages: 92 Median : 0.00 ## Mean : 57.78 One_Story_PUD_1946_and_Newer : 53 Mean : 37.97 ## 3rd Qu.: 78.00 Duplex_All_Styles_and_Ages : 40 3rd Qu.: 0.00 ## Max. :313.00 One_Story_1945_and_Older : 36 Max. :8300.00 ## (Other) :154 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 877 1st Qu.: 0.0 ## Median : 27.00 Median : 6.000 Median :1092 Median : 0.0 ## Mean : 49.93 Mean : 6.413 Mean :1171 Mean : 319.6 ## 3rd Qu.: 72.00 3rd Qu.: 7.000 3rd Qu.:1426 3rd Qu.: 682.0 ## Max. :742.00 Max. :12.000 Max. :4692 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2010 ## nearZeroVar(ames, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## Sale_Price 1.000000 55.7321226 FALSE FALSE ## Gr_Liv_Area 1.333333 62.9965948 FALSE FALSE ## Garage_Type 2.196581 0.7945516 FALSE FALSE ## Garage_Cars 1.970213 0.5675369 FALSE FALSE ## Garage_Area 2.250000 38.0249716 FALSE FALSE ## Street 219.250000 0.2270148 FALSE TRUE ## Utilities 880.000000 0.2270148 FALSE TRUE ## Pool_Area 876.000000 0.6810443 FALSE TRUE ## Neighborhood 1.476744 2.9511918 FALSE FALSE ## Screen_Porch 199.750000 6.6969353 FALSE TRUE ## Overall_Qual 1.119816 1.1350738 FALSE FALSE ## Lot_Area 1.071429 79.7956867 FALSE FALSE ## Lot_Frontage 1.617021 11.5777526 FALSE FALSE ## MS_SubClass 1.959064 1.7026107 FALSE FALSE ## Misc_Val 141.833333 1.9296254 FALSE TRUE ## Open_Porch_SF 23.176471 19.2962543 FALSE FALSE ## TotRms_AbvGrd 1.311224 1.2485812 FALSE FALSE ## First_Flr_SF 1.777778 63.7911464 FALSE FALSE ## Second_Flr_SF 64.250000 31.3280363 FALSE FALSE ## Year_Built 1.116279 12.0317821 FALSE FALSE levels(ames$Overall_Qual) ## [1] &quot;Above_Average&quot; &quot;Average&quot; &quot;Below_Average&quot; &quot;Excellent&quot; ## [5] &quot;Fair&quot; &quot;Good&quot; &quot;Poor&quot; &quot;Very_Excellent&quot; ## [9] &quot;Very_Good&quot; &quot;Very_Poor&quot; # relevel the levels ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) levels(ames$Overall_Qual) # the levels are properly ordered ## [1] &quot;Very_Poor&quot; &quot;Poor&quot; &quot;Fair&quot; &quot;Below_Average&quot; ## [5] &quot;Average&quot; &quot;Above_Average&quot; &quot;Good&quot; &quot;Very_Good&quot; ## [9] &quot;Excellent&quot; &quot;Very_Excellent&quot; Finding that there are 19 features in total and 13 of them are numerical variables, 6 of them are categorical variables. For the ordinal categorical variables Overall_Qual, finding that the level is not in order, hence, we releveled the level for this variable. Further, there’re 113 NA values and 5 nzv features in the whole dataset. Those are things we need to deal within blueprint. 2.1.2 Data Spliting set.seed(013123) index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # consider 80-20 split ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data # investigate nominal categorical predictors ames_train %&gt;% count(Neighborhood) %&gt;% arrange(n) # check frequency of categories ## # A tibble: 26 × 2 ## Neighborhood n ## &lt;fct&gt; &lt;int&gt; ## 1 Northpark_Villa 2 ## 2 Blueste 2 ## 3 Greens 4 ## 4 Veenker 6 ## 5 Bloomington_Heights 7 ## 6 Briardale 8 ## 7 Clear_Creek 11 ## 8 Meadow_Village 12 ## 9 Stone_Brook 13 ## 10 Timberland 14 ## # … with 16 more rows "],["recipe-and-blueprint.html", "2.2 Recipe and Blueprint", " 2.2 Recipe and Blueprint After all preprocessing steps have been decided set up the overall blueprint. # set up recipe ames_recipe &lt;- recipe(Sale_Price~., data = ames_train) # specify feature engineering steps ames_blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_integer(Overall_Qual) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% step_dummy(all_nominal(), one_hot = FALSE) Within the blueprint, we first filter out the zv/nzv predictors, and impute the missing entries. Then, we do numeric conversion of level of ordinal categorical variable Overall_Qual. Following with the data standardization step and lumping step. Finally, do the one-hot/dummy encode of nominal categorical variables. # estimate feature engineering parameters based on training data ames_prepare &lt;- prep(ames_blueprint, data = ames_train) # apply the blueprint to training data for building final/optimal model ames_baked_train &lt;- bake(ames_prepare, new_data = ames_train) # apply the blueprint to test data for future use ames_baked_test &lt;- bake(ames_prepare, new_data = ames_test) "],["analysis.html", "2.3 Analysis", " 2.3 Analysis 2.3.1 Bulid Model set.seed(111) # implement 5-fold CV repeated 5 times cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats=5) # set tunegrid tunegrid &lt;- data.frame(method = &quot;GCV.Cp&quot;, select = TRUE) # GAM model ames_model&lt;-train(ames_blueprint, data = ames_train, method = &quot;gam&quot;, tuneGrid = tunegrid, trControl = cv_specs) We implement 5 fold CV with 5 repeats here to ensure the model is not overfitting and is generalizable to new data, which is very important for GAM model. For the model, Method = \"gam\" indicates the smoothing parameter estimation method, GCV for models with unknown scale parameter and Mallows’ Cp/UBRE/AIC for models with known scale; select=TRUE adds extra penalty so that the smoothing parameter estimation can completely remove terms from the model 2.3.2 Model’s Performance ames_model$results$RMSE ## [1] 70421.17 ames_model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + ## Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + ## Neighborhood_Somerset + Neighborhood_Northridge_Heights + ## Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + ## MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + ## MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + ## Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + ## s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + ## s(First_Flr_SF) + s(Lot_Area) ## ## Estimated degrees of freedom: ## 0.961 0.000 3.936 1.065 1.974 3.105 6.489 ## 8.662 7.680 total = 50.87 ## ## GCV score: 722968462 Here we could see that the RMSE is 70421.17, which is pretty high, probably due to the size of the dataset being big. The estimated degrees of freedom indicate the complexity of the model, and in this case, the EDF ranges from 0 to 8.662, with a total EDF of 50.87. The EDF values for the smoothing functions indicate the effective number of parameters used to fit the data, and higher EDF values generally indicate more flexible, complex models. The GCV score is used as a measure of the model’s performance because it provides a balance between the model’s fit to the data and its complexity. We could also look at the formula of the final model generated from training. It shows that out of the 53 variables, only 9 of the variables have a non-parametric relationship with the response. This could also be the reason for the high RMSE. "],["final-model.html", "2.4 Final Model", " 2.4 Final Model 2.4.1 Prediction on Test Dataset # build final model ames_final_model &lt;- gam(Sale_Price~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + s(First_Flr_SF) + s(Lot_Area),data=ames_baked_train) ames_final_model_preds&lt;- predict(object = ames_final_model, newdata = ames_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((ames_final_model_preds- ames_baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 195576.6 Using the formula provided from training, we tried to fit the GAM model. The RMSE of the final model is 195576.6, compared to 70421.17 previously. Obviously, this is not ideal, since now the RMSE is more than two times higher. Why? plot(ames_final_model, residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;,pch = 1, cex = 0.5,pages = 1) Here we have the residual plot of those variables that had applied splines function, and the blue shades represent the 95% confidence interval. From the graphs for Gr_Liv_Area and Lot_Area, we could see that the tails are drawn to the noises, letting it change the slope, which should not be happening. In order to create a well-fit final model, we have to recognize what is wrong with out current model. First, let’s look at the several plots gam.check had produced. set.seed(1293) gam.check(ames_final_model) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 26 iterations. ## The RMS GCV score gradient at convergence was 17.03165 . ## The Hessian was positive definite. ## Model rank = 98 / 98 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(TotRms_AbvGrd) 9.00 1.00 1.05 0.865 ## s(Lot_Frontage) 9.00 1.00 0.97 0.200 ## s(Year_Built) 9.00 3.94 0.92 0.015 * ## s(Open_Porch_SF) 9.00 1.00 0.91 &lt;2e-16 *** ## s(Second_Flr_SF) 9.00 1.00 0.95 0.085 . ## s(Garage_Area) 9.00 3.62 0.98 0.295 ## s(Gr_Liv_Area) 9.00 9.00 1.00 0.560 ## s(First_Flr_SF) 9.00 6.48 1.09 0.970 ## s(Lot_Area) 9.00 7.81 1.05 0.930 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the Q-Q plot, we could see that there is a clear trend to the residuals; from the residual values plot, we could see that there’s a cluster of points, and maybe a negative slope; from the histogram, we could see that there is a bell-shaped curve; and from the response vs. fitted values plot, we could see that the values are fitted pretty good. We should also look at the table generated from gam.check. The table shows the k value, effective degrees of freedom, test statistics, and p-value of each basis functions. The k value is very similar to the “knots” in MARS, representing how many basis function is used to fit the model, and could be used as the smoothing parameter. Another smoothing parameter we would use is λ, since \\(FIT=Likelihood-λ*Wiggliness\\) is how the fit of GAM model is calculated. The likelihood represents how well the model captures patterns in the data, and wiggliness represents the complexity of the model. As described in the model, when the p-value is too low, or if the k-index is below 1, it’s possible that we need to increase the size of the basis function. In the table above, we could see that Year_Built, Open_porch_SF, and Second_Flr_SF all had p-values below 0.1. # build final model ames_final_model2 &lt;- gam(Sale_Price~Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + s(Garage_Cars,sp=0.1,k=2) + s(Overall_Qual,k=3) + s(TotRms_AbvGrd,sp=0.01,k=11) + s(Lot_Frontage,sp=0.1,k=7) +s(Year_Built,sp=0.0001,k=29) + s(Open_Porch_SF,sp=0.6,k=9) + s(Second_Flr_SF,sp=0.9,k=7) + s(Garage_Area,sp=1,k=5) + s(Gr_Liv_Area,sp=10,k=6) + s(First_Flr_SF,sp=0.001,k=3) + s(Lot_Area,sp=1,k=3) ,data=ames_baked_train) ## Warning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible After seeing where the problem was, we started using the smoothing parameter to tune the variables. From the formula \\(FIT=Likelihood-λ*Wiggliness\\), we know that, the more wiggly (complex) we want the variable to fit, the smaller λ needs to be (between 0 and 1), and if we want the fit to be less wiggly, λ has to be bigger than 1. plot(ames_final_model,select=c(7), residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;) For example, above is the plot from the first model of the variable Gr_Liv_Area. We can see that, when the above ground living area was increasing, the trend of the price goes up when there are more data, and when there is less data, the price started dramatically. This disobeys the common sense, since the bigger the house is, the more expensive the house should be. plot(ames_final_model2,select=c(9), residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;) This is the reason why we need to use λ and tune the variable. For the second model, we chose a λ of 10, which means that we don’t want the trend to be wiggly, and it should be as simple as possible, which also prevents the line of best fit to be drawn by the noises. As we can see from the graph above, the price is now predicted to increase with the above ground area. set.seed(12984) gam.check(ames_final_model2) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 8 iterations. ## The RMS GCV score gradient at convergence was 3093.98 . ## The Hessian was positive definite. ## Model rank = 90 / 90 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(Garage_Cars) 2.00 1.81 1.03 0.77 ## s(Overall_Qual) 2.00 1.95 1.04 0.91 ## s(TotRms_AbvGrd) 10.00 6.24 0.99 0.30 ## s(Lot_Frontage) 6.00 3.43 0.98 0.28 ## s(Year_Built) 28.00 25.78 0.95 0.12 ## s(Open_Porch_SF) 8.00 2.53 0.97 0.18 ## s(Second_Flr_SF) 6.00 2.46 0.98 0.23 ## s(Garage_Area) 4.00 1.80 1.00 0.48 ## s(Gr_Liv_Area) 5.00 1.08 0.97 0.20 ## s(First_Flr_SF) 2.00 1.97 1.00 0.47 ## s(Lot_Area) 2.00 1.03 1.02 0.74 After tuning, we can see that now the basis dimension for all variables have p-values of bigger than 0.1. ames_final_model_preds2&lt;- predict(object = ames_final_model2, newdata = ames_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((ames_final_model_preds2- ames_baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 28781.85 Here, we can also see that the RMSE value had decreased to 28781.85, compared to 195576.6 from the first model. "]]
