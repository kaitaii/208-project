[["index.html", "CMSC 208 Final Project 1 Introduction", " CMSC 208 Final Project Yi Lu, Shiyi Yang March 12, 2023 01:49 UTC 1 Introduction Generalized Additive Models is a powerful extension of the linear regression model that can capture complex nonlinear relationships between predictors and response variables. Unlike linear regression, GAMs allow for non-linear function of each variable, while maintaining their additivity. Instead of \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2+...+\\epsilon\\), GAMs have a function of \\(y = \\beta_0 + f_1(x_1) + f_2(x_2) + ... +\\epsilon\\), where \\(f(xi)\\) represent a (smooth) non-linear function. GAMs can also model interactions between variables and handle different types of data, including continuous, categorical, and ordinal variables. In this project, we are trying to imply this new model we learned and explored the performance of it on two datasets, ames and boston, which have different features. However, our initial results on the ames dataset were a bit disappointing, with a very large Root Mean Squared Error (RMSE) on both the training and test datasets, indicating poor performance. To address this issue, we further investigated the model and its hyperparameters, and find another dataset boston with more complexity to improve GAMs performance. We present our findings for using GAMs on these datasets in this report. "],["ames-housing-dataset-example.html", "2 Ames Housing Dataset Example", " 2 Ames Housing Dataset Example Data Description: First, we use the ames dataset that we used in class, since we want to first learn how to apply this model and then do further investigation. "],["preparation-steps.html", "2.1 Preparation Steps", " 2.1 Preparation Steps 2.1.1 Data Investigation Just as before, we first investigate the dataset to see if there’s any missing entries, non-numeric variables, or ZV/NZV features. glimpse(ames) ## Rows: 881 ## Columns: 20 ## $ Sale_Price &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, … ## $ Gr_Liv_Area &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, … ## $ Garage_Cars &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,… ## $ Garage_Area &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Neighborhood &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest… ## $ Screen_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Overall_Qual &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average… ## $ Lot_Area &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101… ## $ Lot_Frontage &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6… ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_… ## $ Misc_Val &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,… ## $ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7… ## $ First_Flr_SF &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11… ## $ Year_Built &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20… sum(is.na(ames)) ## [1] 113 summary(ames) ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 34900 Min. : 334 Attchd :514 Min. :0.000 ## 1st Qu.:129500 1st Qu.:1118 Basment : 10 1st Qu.:1.000 ## Median :160000 Median :1442 BuiltIn : 55 Median :2.000 ## Mean :181115 Mean :1495 CarPort : 5 Mean :1.762 ## 3rd Qu.:213500 3rd Qu.:1728 Detchd :234 3rd Qu.:2.000 ## Max. :755000 Max. :5642 More_Than_Two_Types: 9 Max. :4.000 ## NA&#39;s :113 No_Garage : 54 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:880 Min. : 0.00 ## 1st Qu.: 324.0 Pave:877 NoSeWa: 0 1st Qu.: 0.00 ## Median : 480.0 NoSewr: 1 Median : 0.00 ## Mean : 476.5 Mean : 2.41 ## 3rd Qu.: 592.0 3rd Qu.: 0.00 ## Max. :1418.0 Max. :576.00 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames :127 Min. : 0.00 Average :243 Min. : 1300 ## College_Creek : 86 1st Qu.: 0.00 Above_Average:217 1st Qu.: 7449 ## Old_Town : 83 Median : 0.00 Good :177 Median : 9512 ## Northridge_Heights: 52 Mean : 18.11 Very_Good : 99 Mean : 10105 ## Somerset : 50 3rd Qu.: 0.00 Below_Average: 83 3rd Qu.: 11526 ## Edwards : 49 Max. :490.00 Excellent : 38 Max. :159000 ## (Other) :434 (Other) : 24 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.00 One_Story_1946_and_Newer_All_Styles :335 Min. : 0.00 ## 1st Qu.: 43.00 Two_Story_1946_and_Newer :171 1st Qu.: 0.00 ## Median : 63.00 One_and_Half_Story_Finished_All_Ages: 92 Median : 0.00 ## Mean : 57.78 One_Story_PUD_1946_and_Newer : 53 Mean : 37.97 ## 3rd Qu.: 78.00 Duplex_All_Styles_and_Ages : 40 3rd Qu.: 0.00 ## Max. :313.00 One_Story_1945_and_Older : 36 Max. :8300.00 ## (Other) :154 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 877 1st Qu.: 0.0 ## Median : 27.00 Median : 6.000 Median :1092 Median : 0.0 ## Mean : 49.93 Mean : 6.413 Mean :1171 Mean : 319.6 ## 3rd Qu.: 72.00 3rd Qu.: 7.000 3rd Qu.:1426 3rd Qu.: 682.0 ## Max. :742.00 Max. :12.000 Max. :4692 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2010 ## nearZeroVar(ames, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## Sale_Price 1.000000 55.7321226 FALSE FALSE ## Gr_Liv_Area 1.333333 62.9965948 FALSE FALSE ## Garage_Type 2.196581 0.7945516 FALSE FALSE ## Garage_Cars 1.970213 0.5675369 FALSE FALSE ## Garage_Area 2.250000 38.0249716 FALSE FALSE ## Street 219.250000 0.2270148 FALSE TRUE ## Utilities 880.000000 0.2270148 FALSE TRUE ## Pool_Area 876.000000 0.6810443 FALSE TRUE ## Neighborhood 1.476744 2.9511918 FALSE FALSE ## Screen_Porch 199.750000 6.6969353 FALSE TRUE ## Overall_Qual 1.119816 1.1350738 FALSE FALSE ## Lot_Area 1.071429 79.7956867 FALSE FALSE ## Lot_Frontage 1.617021 11.5777526 FALSE FALSE ## MS_SubClass 1.959064 1.7026107 FALSE FALSE ## Misc_Val 141.833333 1.9296254 FALSE TRUE ## Open_Porch_SF 23.176471 19.2962543 FALSE FALSE ## TotRms_AbvGrd 1.311224 1.2485812 FALSE FALSE ## First_Flr_SF 1.777778 63.7911464 FALSE FALSE ## Second_Flr_SF 64.250000 31.3280363 FALSE FALSE ## Year_Built 1.116279 12.0317821 FALSE FALSE levels(ames$Overall_Qual) ## [1] &quot;Above_Average&quot; &quot;Average&quot; &quot;Below_Average&quot; &quot;Excellent&quot; ## [5] &quot;Fair&quot; &quot;Good&quot; &quot;Poor&quot; &quot;Very_Excellent&quot; ## [9] &quot;Very_Good&quot; &quot;Very_Poor&quot; # relevel the levels ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) levels(ames$Overall_Qual) # the levels are properly ordered ## [1] &quot;Very_Poor&quot; &quot;Poor&quot; &quot;Fair&quot; &quot;Below_Average&quot; ## [5] &quot;Average&quot; &quot;Above_Average&quot; &quot;Good&quot; &quot;Very_Good&quot; ## [9] &quot;Excellent&quot; &quot;Very_Excellent&quot; Finding that there are 19 features in total and 13 of them are numerical variables, 6 of them are categorical variables. For the ordinal categorical variables Overall_Qual, finding that the level is not in order, hence, we relevel the level for this variable. Further, there’re 113 NA values and 5 nzv features in the whole dataset. Those are things we need to deal within blueprint. 2.1.2 Data Spliting set.seed(013123) index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # consider 80-20 split ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data # investigate nominal categorical predictors ames_train %&gt;% count(Neighborhood) %&gt;% arrange(n) # check frequency of categories ## # A tibble: 26 × 2 ## Neighborhood n ## &lt;fct&gt; &lt;int&gt; ## 1 Northpark_Villa 2 ## 2 Blueste 2 ## 3 Greens 4 ## 4 Veenker 6 ## 5 Bloomington_Heights 7 ## 6 Briardale 8 ## 7 Clear_Creek 11 ## 8 Meadow_Village 12 ## 9 Stone_Brook 13 ## 10 Timberland 14 ## # … with 16 more rows "],["recipt-and-blueprint.html", "2.2 Recipt and Blueprint", " 2.2 Recipt and Blueprint After all preprocessing steps have been decided set up the overall blueprint. # set up recipe ames_recipe &lt;- recipe(Sale_Price~., data = ames_train) # specify feature engineering steps ames_blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_integer(Overall_Qual) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% step_dummy(all_nominal(), one_hot = FALSE) Within the blueprint, we first filter out the zv/nzv predictors, and impute the missing entries. Then, we do numeric conversion of level of ordinal categorical variable Overall_Qual. Following with the data standardization step and lumping step. Finally, do the one-hot/dummy encode of nominal categorical variables. # estimate feature engineering parameters based on training data ames_prepare &lt;- prep(ames_blueprint, data = ames_train) # apply the blueprint to training data for building final/optimal model ames_baked_train &lt;- bake(ames_prepare, new_data = ames_train) # apply the blueprint to test data for future use ames_baked_test &lt;- bake(ames_prepare, new_data = ames_test) "],["analysis.html", "2.3 Analysis", " 2.3 Analysis 2.3.1 Bulid Model set.seed(111) # implement 5-fold CV repeated 5 times cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats=5) # set tunegrid tunegrid &lt;- data.frame(method = &quot;GCV.Cp&quot;, select = TRUE) # GAM model ames_model&lt;-train(ames_blueprint, data = ames_train, method = &quot;gam&quot;, tuneGrid = tunegrid, trControl = cv_specs) We implement 5 fold CV with 5 repeats here to ensure the model is not overfitting and is generalizable to new data, which is very important for GAM model. For the model, Method = \"gam\" indicates the smoothing parameter estimation method, GCV for models with unknown scale parameter and Mallows’ Cp/UBRE/AIC for models with known scale; select=TRUE adds extra penalty so that the smoothing parameter estimation can completely remove terms from the model 2.3.2 Model’s Performance ames_model$results$RMSE ## [1] 70421.17 ames_model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + ## Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + ## Neighborhood_Somerset + Neighborhood_Northridge_Heights + ## Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + ## MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + ## MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + ## Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + ## s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + ## s(First_Flr_SF) + s(Lot_Area) ## ## Estimated degrees of freedom: ## 0.961 0.000 3.936 1.065 1.974 3.105 6.489 ## 8.662 7.680 total = 50.87 ## ## GCV score: 722968462 Here we could see that the RMSE is 70421.17, which is pretty high, probably due to the size of the dataset being big. We could also look at the formula of the final model generated from training. It shows that out of the 53 variables, only 9 of the variables have a non-parametric relationship with the response. This could also be the reason for the high RMSE. "],["final-model.html", "2.4 Final Model", " 2.4 Final Model 2.4.1 Prediction on Test Dataset # build final model ames_final_model &lt;- gam(Sale_Price~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + s(First_Flr_SF) + s(Lot_Area),data=ames_baked_train) ames_final_model_preds&lt;- predict(object = ames_final_model, newdata = ames_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((ames_final_model_preds- ames_baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 195576.6 Using the formula provided from training, we tried to fit the GAM model. The RMSE of the final model is 195576.6, compared to 70421.17 previously. Obviously, this is not ideal, since now the RMSE is more than two times higher. Why? plot(ames_final_model, residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;,pch = 1, cex = 0.5) Here we have the residual plot of those variables that had applied splines function, and the blue shades represent the 95% confidence interval. From the graphs for Gr_Liv_Area and Lot_Area, we could see that the tails are drawn to the noises, letting it change the slope, which should not be happening. In order to create a well-fit final model, we have to recognize what is wrong with out current model. First, let’s look at the several plots gam.check had produced. gam.check(ames_final_model) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 26 iterations. ## The RMS GCV score gradient at convergence was 17.03165 . ## The Hessian was positive definite. ## Model rank = 98 / 98 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(TotRms_AbvGrd) 9.00 1.00 1.05 0.890 ## s(Lot_Frontage) 9.00 1.00 0.97 0.200 ## s(Year_Built) 9.00 3.94 0.92 0.010 ** ## s(Open_Porch_SF) 9.00 1.00 0.91 0.005 ** ## s(Second_Flr_SF) 9.00 1.00 0.95 0.075 . ## s(Garage_Area) 9.00 3.62 0.98 0.220 ## s(Gr_Liv_Area) 9.00 9.00 1.00 0.485 ## s(First_Flr_SF) 9.00 6.48 1.09 1.000 ## s(Lot_Area) 9.00 7.81 1.05 0.930 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the Q-Q plot, we could see that there is a clear trend to the residuals; from the residual values plot, we could see that there’s a cluster of points, and maybe a negative slope; from the histogram, we could see that there is a bell-shaped curve; and from the response vs. fitted values plot, we could see that the values are fitted pretty good. We should also look at the table generated from gam.check. The table shows the k value, effective degrees of freedom, test statistics, and p-value of each basis functions. The k value is very similar to the “knots” in MARS, representing how many basic function is used to fit the model. What could be use the smoothing parameters λ and k # build final model ames_final_model2 &lt;- gam(Sale_Price~Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + s(Garage_Cars,sp=0.1,k=2) + s(Overall_Qual,k=3) + s(TotRms_AbvGrd,sp=0.01,k=11) + s(Lot_Frontage,sp=0.1,k=6) +s(Year_Built,sp=0.0001,k=24) + s(Open_Porch_SF,sp=0.15,k=9) + s(Second_Flr_SF,sp=0.9,k=7) + s(Garage_Area,sp=1,k=5) + s(Gr_Liv_Area,sp=0.4,k=6) + s(First_Flr_SF,sp=0.001,k=3) + s(Lot_Area,sp=1,k=3) ,data=ames_baked_train) ## Warning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible ames_final_model_preds2&lt;- predict(object = ames_final_model2, newdata = ames_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((ames_final_model_preds2- ames_baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 28805.21 plot(ames_final_model2, residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;) gam.check(ames_final_model2) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 8 iterations. ## The RMS GCV score gradient at convergence was 1873.426 . ## The Hessian was positive definite. ## Model rank = 84 / 84 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(Garage_Cars) 2.00 1.81 1.02 0.68 ## s(Overall_Qual) 2.00 1.94 1.03 0.78 ## s(TotRms_AbvGrd) 10.00 6.24 0.98 0.36 ## s(Lot_Frontage) 5.00 3.37 0.98 0.27 ## s(Year_Built) 23.00 22.02 0.94 0.03 * ## s(Open_Porch_SF) 8.00 3.55 0.97 0.16 ## s(Second_Flr_SF) 6.00 2.38 0.96 0.17 ## s(Garage_Area) 4.00 1.81 1.00 0.43 ## s(Gr_Liv_Area) 5.00 2.15 0.98 0.33 ## s(First_Flr_SF) 2.00 1.95 1.02 0.64 ## s(Lot_Area) 2.00 1.03 1.01 0.64 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Noticing that the RMSE for both train dataset and test dataset are all large, especailly for test dataset, which means the GAM model didn’t work well for ames housing dataset. In order to show a better performance, we found another dataset, boston housing, to have a try. "],["boston-housing-dataset-example.html", "3 Boston housing dataset Example", " 3 Boston housing dataset Example Data description: Since GAM did not work well with the Ames housing dataset, which output a big RMSE, we decided to use another dataset and to test the GAM method on it again. The new dataset, Boston housing, includes 13 explanatory variables, and the response variable MEDV. The following is the description from Kaggle all variables: Response Variable: MEDV: Median value of owner-occupied homes in $1000’s [k$] Explanatory variables: CRIM: per capita crime rate by town ZN: proportion of residential land zoned for lots over 25,000 sq.ft. INDUS: proportion of non-retail business acres per town CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) NOX: nitric oxides concentration (parts per 10 million) [parts/10M] RM: average number of rooms per dwelling AGE: proportion of owner-occupied units built prior to 1940 DIS: weighted distances to five Boston employment centres RAD: index of accessibility to radial highways TAX: full-value property-tax rate per $10,000 [$/10k] PTRATIO: pupil-teacher ratio by town B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT: % lower status of the population "],["preparation-steps-1.html", "3.1 Preparation Steps", " 3.1 Preparation Steps 3.1.1 Data Investigation Just as before, we first investigate the dataset to see if there’s any missing entries, non-numeric variables, or ZV/NZV features. glimpse(boston) ## Rows: 506 ## Columns: 14 ## $ CRIM &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ ZN &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ INDUS &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ CHAS &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ NOX &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ RM &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ AGE &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ DIS &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ RAD &lt;dbl&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ TAX &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ PTRATIO &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ B &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ LSTAT &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ MEDV &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… summary(boston) ## CRIM ZN INDUS CHAS ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## NOX RM AGE DIS ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## RAD TAX PTRATIO B ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## LSTAT MEDV ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00 sum(is.na(boston)) ## [1] 0 nearZeroVar(boston) ## integer(0) Finding that there are 13 features in total and all of them are numerical variables. There is no NA value in the whole dataset and no zv or nzv features. cor(boston) ## CRIM ZN INDUS CHAS NOX ## CRIM 1.00000000 -0.20046922 0.40658341 -0.055891582 0.42097171 ## ZN -0.20046922 1.00000000 -0.53382819 -0.042696719 -0.51660371 ## INDUS 0.40658341 -0.53382819 1.00000000 0.062938027 0.76365145 ## CHAS -0.05589158 -0.04269672 0.06293803 1.000000000 0.09120281 ## NOX 0.42097171 -0.51660371 0.76365145 0.091202807 1.00000000 ## RM -0.21924670 0.31199059 -0.39167585 0.091251225 -0.30218819 ## AGE 0.35273425 -0.56953734 0.64477851 0.086517774 0.73147010 ## DIS -0.37967009 0.66440822 -0.70802699 -0.099175780 -0.76923011 ## RAD 0.62550515 -0.31194783 0.59512927 -0.007368241 0.61144056 ## TAX 0.58276431 -0.31456332 0.72076018 -0.035586518 0.66802320 ## PTRATIO 0.28994558 -0.39167855 0.38324756 -0.121515174 0.18893268 ## B -0.38506394 0.17552032 -0.35697654 0.048788485 -0.38005064 ## LSTAT 0.45562148 -0.41299457 0.60379972 -0.053929298 0.59087892 ## MEDV -0.38830461 0.36044534 -0.48372516 0.175260177 -0.42732077 ## RM AGE DIS RAD TAX PTRATIO ## CRIM -0.21924670 0.35273425 -0.37967009 0.625505145 0.58276431 0.2899456 ## ZN 0.31199059 -0.56953734 0.66440822 -0.311947826 -0.31456332 -0.3916785 ## INDUS -0.39167585 0.64477851 -0.70802699 0.595129275 0.72076018 0.3832476 ## CHAS 0.09125123 0.08651777 -0.09917578 -0.007368241 -0.03558652 -0.1215152 ## NOX -0.30218819 0.73147010 -0.76923011 0.611440563 0.66802320 0.1889327 ## RM 1.00000000 -0.24026493 0.20524621 -0.209846668 -0.29204783 -0.3555015 ## AGE -0.24026493 1.00000000 -0.74788054 0.456022452 0.50645559 0.2615150 ## DIS 0.20524621 -0.74788054 1.00000000 -0.494587930 -0.53443158 -0.2324705 ## RAD -0.20984667 0.45602245 -0.49458793 1.000000000 0.91022819 0.4647412 ## TAX -0.29204783 0.50645559 -0.53443158 0.910228189 1.00000000 0.4608530 ## PTRATIO -0.35550149 0.26151501 -0.23247054 0.464741179 0.46085304 1.0000000 ## B 0.12806864 -0.27353398 0.29151167 -0.444412816 -0.44180801 -0.1773833 ## LSTAT -0.61380827 0.60233853 -0.49699583 0.488676335 0.54399341 0.3740443 ## MEDV 0.69535995 -0.37695457 0.24992873 -0.381626231 -0.46853593 -0.5077867 ## B LSTAT MEDV ## CRIM -0.38506394 0.4556215 -0.3883046 ## ZN 0.17552032 -0.4129946 0.3604453 ## INDUS -0.35697654 0.6037997 -0.4837252 ## CHAS 0.04878848 -0.0539293 0.1752602 ## NOX -0.38005064 0.5908789 -0.4273208 ## RM 0.12806864 -0.6138083 0.6953599 ## AGE -0.27353398 0.6023385 -0.3769546 ## DIS 0.29151167 -0.4969958 0.2499287 ## RAD -0.44441282 0.4886763 -0.3816262 ## TAX -0.44180801 0.5439934 -0.4685359 ## PTRATIO -0.17738330 0.3740443 -0.5077867 ## B 1.00000000 -0.3660869 0.3334608 ## LSTAT -0.36608690 1.0000000 -0.7376627 ## MEDV 0.33346082 -0.7376627 1.0000000 From the correlation plot, noticing that for variable TAX and INDUS, DIS and INDUS, TAX and INDUS, NOX and INDUS, NOX and AGE, NOX and DIS, AGE and DIS… are highly correlated variable. (We are saying two variables are highly correlated when their correlation value is greater that 0.7) 3.1.2 Data Spliting set.seed(013123) index &lt;- createDataPartition(y = boston$MEDV, p = 0.8, list = FALSE) # consider 80-20 split #Data Spliting boston_train &lt;- boston[index,] # training data boston_test &lt;- boston[-index,] # test data "],["recipt-and-blueprint-1.html", "3.2 Recipt and Blueprint", " 3.2 Recipt and Blueprint # Set up recipe boston_recipe &lt;- recipe(MEDV ~ ., data = boston_train) # set up blueprint boston_blueprint &lt;- boston_recipe %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response) step_scale(all_numeric(), -all_outcomes()) # scale (divide by standard deviation) all numeric predictors Since there’s no missing value, no zv/nzv features and no categorical variable in boston dataset, all we need in blueprint is to standardize the data. # estimate feature engineering parameters based on training data boston_prepare &lt;- prep(boston_blueprint, data = boston_train) # apply the blueprint to training data for building final/optimal model boston_baked_train &lt;- bake(boston_prepare, new_data = boston_train) # apply the blueprint to test data for future use boston_baked_test &lt;- bake(boston_prepare, new_data = boston_test) "],["analysis-1.html", "3.3 Analysis", " 3.3 Analysis 3.3.1 Bulid Model set.seed(111) # implement 5-fold CV with no repeat cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # set tunegrid tunegrid &lt;- data.frame(method = &quot;GCV.Cp&quot;, select = TRUE) # GAM model boston_model &lt;- train(boston_blueprint, data = boston_train, method = &quot;gam&quot;, tuneGrid = data.frame(method = &quot;GCV.Cp&quot;, select = TRUE), trControl = cv_specs) We implement CV with no repeat here to ensure the model is not overfitting and is generalizable to new data, which is very important for GAM model. 3.3.2 Model’s Performance # RMSE boston_model$results$RMSE ## [1] 3.989444 boston_model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + ## s(NOX) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT) + s(CRIM) ## ## Estimated degrees of freedom: ## 0.000 0.927 2.906 5.636 7.931 4.886 0.000 ## 0.889 8.392 5.813 4.421 total = 44.8 ## ## GCV score: 12.95974 From the result, seeing that the RMSE is very small here, around 4, which indicates GAM model works well with train boston dataset. The estimated degrees of freedom indicate the complexity of the model, and in this case, the EDF ranges from 0 to 8.392, with a total EDF of 44.8. The EDF values for the smoothing functions indicate the effective number of parameters used to fit the data, and higher EDF values generally indicate more flexible, complex models. The GCV score is used as a measure of the model’s performance because it provides a balance between the model’s fit to the data and its complexity. "],["final-model-1.html", "3.4 Final Model", " 3.4 Final Model 3.4.1 Prediction on Test Dataset # obtain predictions and test set RMSE boston_final_model &lt;- gam(MEDV ~ CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + s(NOX,sp=0.01) + s(B,sp=8) + s(AGE) + s(DIS,sp=0.01) + s(RM,sp=1) + s(LSTAT) + s(CRIM), data = boston_baked_train) boston_final_model_preds&lt;- predict(object = boston_final_model, newdata = boston_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((boston_final_model_preds - boston_baked_test$MEDV)^2)) # calculate test set RMSE ## [1] 2.587925 The test set RMSE is even smaller, which is a good sign. Comparing with using GAM model for ames dataset, using it for boston dataset seems to be a lot better. The reason to that is the fact that the boston dataset is a lot more complex then the ames dataset. boston_final_model$sp ## s(ZN) s(PTRATIO) s(TAX) s(INDUS) s(AGE) s(LSTAT) ## 4.454571e+09 7.813311e+00 2.252458e-02 3.350616e-02 5.356143e+09 7.878517e-03 ## s(CRIM) ## 3.959924e-02 plot(boston_final_model, residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;, pch = 1) Let’s look at the residual plot for the boston final model. As we can see from above, only two of the graphs seem to have a slope close to 0, and lots of others with non-parametric relationships. This is a sign that the model is way more complex, compared to the ames dataset, where only 3 explanatory variables seemed to have non-parametric relationship with the response. 3.4.2 Only splines? What happens if we only use the variables with splines to predict the response MEDV? As we’ve seen from the attempt in ames, the RMSE of only using splines as the explanatory variables had reduced the RMSE. Would that work the same here? # obtain predictions and test set RMSE boston_final_model2 &lt;- gam(MEDV~ s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + s(NOX) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT) + s(CRIM),data=boston_baked_train) boston_final_model_preds2&lt;- predict(object = boston_final_model2, newdata = boston_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((boston_final_model_preds2 - boston_baked_test$MEDV)^2)) # calculate test set RMSE ## [1] 3.059528 coef(boston_final_model) ## (Intercept) CHAS RAD s(ZN).1 s(ZN).2 ## 2.267715e+01 2.306602e-01 3.696429e+00 -6.171238e-10 -1.192933e-09 ## s(ZN).3 s(ZN).4 s(ZN).5 s(ZN).6 s(ZN).7 ## 7.480646e-10 9.678993e-10 -8.251146e-10 1.004524e-09 6.413294e-10 ## s(ZN).8 s(ZN).9 s(PTRATIO).1 s(PTRATIO).2 s(PTRATIO).3 ## -4.914599e-09 2.564574e-01 5.422391e-02 -1.211415e-01 -7.051639e-02 ## s(PTRATIO).4 s(PTRATIO).5 s(PTRATIO).6 s(PTRATIO).7 s(PTRATIO).8 ## 1.430908e-01 1.405820e-04 1.216246e-01 3.704855e-02 -7.120412e-01 ## s(PTRATIO).9 s(TAX).1 s(TAX).2 s(TAX).3 s(TAX).4 ## -1.715808e+00 2.993991e+00 2.935675e+01 -1.014211e+01 -1.487407e+01 ## s(TAX).5 s(TAX).6 s(TAX).7 s(TAX).8 s(TAX).9 ## -3.938658e+00 -5.852286e+00 -1.916158e+00 -3.806928e+01 -7.516277e+00 ## s(INDUS).1 s(INDUS).2 s(INDUS).3 s(INDUS).4 s(INDUS).5 ## -1.910599e+00 -2.438087e+00 1.212783e+00 -4.881653e-01 3.066354e-01 ## s(INDUS).6 s(INDUS).7 s(INDUS).8 s(INDUS).9 s(NOX).1 ## -2.668014e-02 1.709398e-01 -6.021530e-02 -3.975620e+00 -2.050677e+00 ## s(NOX).2 s(NOX).3 s(NOX).4 s(NOX).5 s(NOX).6 ## -6.460199e+00 -9.963736e-01 6.063462e+00 5.378468e+00 -7.239073e+00 ## s(NOX).7 s(NOX).8 s(NOX).9 s(B).1 s(B).2 ## -7.317250e-01 2.300479e+01 -1.525602e+00 7.301973e-02 3.907277e-01 ## s(B).3 s(B).4 s(B).5 s(B).6 s(B).7 ## -2.307254e-01 3.563254e-01 1.252734e-01 -3.444455e-01 1.429641e-01 ## s(B).8 s(B).9 s(AGE).1 s(AGE).2 s(AGE).3 ## 1.385920e+00 3.345819e-01 2.432130e-10 9.770529e-11 -5.405501e-12 ## s(AGE).4 s(AGE).5 s(AGE).6 s(AGE).7 s(AGE).8 ## 1.435740e-10 -1.371786e-11 1.244389e-10 2.127511e-13 5.642292e-10 ## s(AGE).9 s(DIS).1 s(DIS).2 s(DIS).3 s(DIS).4 ## 1.459007e-01 -4.837707e+00 3.956269e+00 -3.199652e+00 -2.251719e+00 ## s(DIS).5 s(DIS).6 s(DIS).7 s(DIS).8 s(DIS).9 ## 2.483504e+00 -2.030837e+00 4.740723e-01 3.452745e+00 -1.522897e+01 ## s(RM).1 s(RM).2 s(RM).3 s(RM).4 s(RM).5 ## -3.470665e-01 4.647636e-01 3.080395e-02 1.835072e-01 1.947112e-01 ## s(RM).6 s(RM).7 s(RM).8 s(RM).9 s(LSTAT).1 ## -2.292482e-01 -1.337610e-01 -3.438966e+00 1.995920e+00 2.850793e+00 ## s(LSTAT).2 s(LSTAT).3 s(LSTAT).4 s(LSTAT).5 s(LSTAT).6 ## -1.276700e+01 1.704206e+00 6.374418e+00 3.064578e+00 6.886423e+00 ## s(LSTAT).7 s(LSTAT).8 s(LSTAT).9 s(CRIM).1 s(CRIM).2 ## 1.410919e+00 -2.153757e+01 -1.198140e+01 7.184804e+00 1.449372e+01 ## s(CRIM).3 s(CRIM).4 s(CRIM).5 s(CRIM).6 s(CRIM).7 ## -1.802656e-01 -7.457285e+00 5.760114e+00 6.780832e+00 -1.334425e+00 ## s(CRIM).8 s(CRIM).9 ## 1.941030e+01 -3.077602e+00 summary(boston_final_model) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## MEDV ~ CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + ## s(NOX, sp = 0.01) + s(B, sp = 8) + s(AGE) + s(DIS, sp = 0.01) + ## s(RM, sp = 1) + s(LSTAT) + s(CRIM) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.6771 0.1689 134.25 &lt; 2e-16 *** ## CHAS 0.2307 0.1906 1.21 0.22711 ## RAD 3.6964 1.2404 2.98 0.00308 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(ZN) 1.000 1.000 0.458 0.4991 ## s(PTRATIO) 1.504 1.832 18.817 8.43e-07 *** ## s(TAX) 6.470 7.469 5.684 2.51e-06 *** ## s(INDUS) 4.462 5.370 2.015 0.0416 * ## s(NOX) 7.261 8.166 10.278 &lt; 2e-16 *** ## s(B) 1.594 1.954 2.543 0.1170 ## s(AGE) 1.000 1.000 0.159 0.6904 ## s(DIS) 7.165 8.173 5.533 2.13e-06 *** ## s(RM) 2.718 3.482 33.901 &lt; 2e-16 *** ## s(LSTAT) 7.686 8.528 21.732 &lt; 2e-16 *** ## s(CRIM) 4.694 5.582 7.410 9.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.866 Deviance explained = 88.2% ## GCV = 13.187 Scale est. = 11.614 n = 407 "],["conclusion.html", "4 Conclusion", " 4 Conclusion "]]
