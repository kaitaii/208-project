[["index.html", "CMSC 208 Final Project 1 Introduction", " CMSC 208 Final Project Yi Lu, Shiyi Yang March 12, 2023 20:26 UTC 1 Introduction Generalized Additive Models is a powerful extension of the linear regression model that can capture complex nonlinear relationships between predictors and response variables. Unlike linear regression, GAMs allow for non-linear function of each variable, while maintaining their additivity. Instead of \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2+...+\\epsilon\\), GAMs have a function of \\(y = \\beta_0 + f_1(x_1) + f_2(x_2) + ... +\\epsilon\\), where \\(f(x_i)\\) represent a (smooth) non-linear function. GAMs can also model interactions between variables and handle different types of data, including continuous, categorical, and ordinal variables. In this project, we are trying to imply this new model we learned and explored the performance of it on two datasets, ames and boston, which have different features and complexity. However, our initial results on the ames dataset were a bit disappointing, with a very large Root Mean Squared Error (RMSE) on both the training and test datasets, indicating poor performance. To address this issue, we further investigated the model and its hyper-parameters, and implemented the hyper-parameters on both ames and boston datasets. We present our findings for using GAMs on these datasets in this report. "],["ames-housing-dataset-example.html", "2 Ames Housing Dataset Example", " 2 Ames Housing Dataset Example Data Description: First, we use the ames dataset that we used in class, since we want to first learn how to apply this model and then do further investigation. Response variable: Sale_Price: Property sale price in USD Explanatory variable: Gr_Liv_Area: Above grade (ground) living area square feet Garage_Type: Garage location Garage_Cars: Size of garage in car capacity Garage_Area: Size of garage in square feet Street: Type of road access to property Utilities: Type of utilities available Pool_Area: Pool area in square feet Neighborhood: Physical locations within Ames city limits Screen_Porch: Screen porch area in square feet Overall_Qual: Rates the overall material and finish of the house Lot_Area: Lot size in square feet Lot_Frontage: Linear feet of street connected to property MS_SubClass: Identifies the type of dwelling involved in the sale. Misc_Val: Dollar value of miscellaneous feature Open_Porch_SF: Open porch area in square feet TotRms_AbvGrd: Total rooms above grade (does not include bathrooms) · First_Flr_SF: First Floor square feet Second_Flr_SF: Second floor square feet Year_Built: Original construction date "],["preparation-steps.html", "2.1 Preparation Steps", " 2.1 Preparation Steps 2.1.1 Data Investigation Just as before, we first investigate the dataset to see if there’s any missing entries, non-numeric variables, or ZV/NZV features. glimpse(ames) ## Rows: 881 ## Columns: 20 ## $ Sale_Price &lt;int&gt; 244000, 213500, 185000, 394432, 190000, 149000, 149900, … ## $ Gr_Liv_Area &lt;int&gt; 2110, 1338, 1187, 1856, 1844, NA, NA, 1069, 1940, 1544, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, … ## $ Garage_Cars &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 3, 3, 2, 3, 3, 2, 2, 2, 3, 2, 2,… ## $ Garage_Area &lt;dbl&gt; 522, 582, 420, 834, 546, 480, 500, 440, 606, 868, 532, 7… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pa… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Neighborhood &lt;fct&gt; North_Ames, Stone_Brook, Gilbert, Stone_Brook, Northwest… ## $ Screen_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Overall_Qual &lt;fct&gt; Good, Very_Good, Above_Average, Excellent, Above_Average… ## $ Lot_Area &lt;int&gt; 11160, 4920, 7980, 11394, 11751, 11241, 12537, 4043, 101… ## $ Lot_Frontage &lt;dbl&gt; 93, 41, 0, 88, 105, 0, 0, 53, 83, 94, 95, 90, 105, 61, 6… ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_PUD_1946_… ## $ Misc_Val &lt;int&gt; 0, 0, 500, 0, 0, 700, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Open_Porch_SF &lt;int&gt; 0, 0, 21, 0, 122, 0, 0, 55, 95, 35, 70, 74, 130, 82, 48,… ## $ TotRms_AbvGrd &lt;int&gt; 8, 6, 6, 8, 7, 5, 6, 4, 8, 7, 7, 7, 7, 6, 7, 7, 10, 7, 7… ## $ First_Flr_SF &lt;int&gt; 2110, 1338, 1187, 1856, 1844, 1004, 1078, 1069, 1940, 15… ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 563, 0, 886, 656, 11… ## $ Year_Built &lt;int&gt; 1968, 2001, 1992, 2010, 1977, 1970, 1971, 1977, 2009, 20… sum(is.na(ames)) ## [1] 113 summary(ames) ## Sale_Price Gr_Liv_Area Garage_Type Garage_Cars ## Min. : 34900 Min. : 334 Attchd :514 Min. :0.000 ## 1st Qu.:129500 1st Qu.:1118 Basment : 10 1st Qu.:1.000 ## Median :160000 Median :1442 BuiltIn : 55 Median :2.000 ## Mean :181115 Mean :1495 CarPort : 5 Mean :1.762 ## 3rd Qu.:213500 3rd Qu.:1728 Detchd :234 3rd Qu.:2.000 ## Max. :755000 Max. :5642 More_Than_Two_Types: 9 Max. :4.000 ## NA&#39;s :113 No_Garage : 54 ## Garage_Area Street Utilities Pool_Area ## Min. : 0.0 Grvl: 4 AllPub:880 Min. : 0.00 ## 1st Qu.: 324.0 Pave:877 NoSeWa: 0 1st Qu.: 0.00 ## Median : 480.0 NoSewr: 1 Median : 0.00 ## Mean : 476.5 Mean : 2.41 ## 3rd Qu.: 592.0 3rd Qu.: 0.00 ## Max. :1418.0 Max. :576.00 ## ## Neighborhood Screen_Porch Overall_Qual Lot_Area ## North_Ames :127 Min. : 0.00 Average :243 Min. : 1300 ## College_Creek : 86 1st Qu.: 0.00 Above_Average:217 1st Qu.: 7449 ## Old_Town : 83 Median : 0.00 Good :177 Median : 9512 ## Northridge_Heights: 52 Mean : 18.11 Very_Good : 99 Mean : 10105 ## Somerset : 50 3rd Qu.: 0.00 Below_Average: 83 3rd Qu.: 11526 ## Edwards : 49 Max. :490.00 Excellent : 38 Max. :159000 ## (Other) :434 (Other) : 24 ## Lot_Frontage MS_SubClass Misc_Val ## Min. : 0.00 One_Story_1946_and_Newer_All_Styles :335 Min. : 0.00 ## 1st Qu.: 43.00 Two_Story_1946_and_Newer :171 1st Qu.: 0.00 ## Median : 63.00 One_and_Half_Story_Finished_All_Ages: 92 Median : 0.00 ## Mean : 57.78 One_Story_PUD_1946_and_Newer : 53 Mean : 37.97 ## 3rd Qu.: 78.00 Duplex_All_Styles_and_Ages : 40 3rd Qu.: 0.00 ## Max. :313.00 One_Story_1945_and_Older : 36 Max. :8300.00 ## (Other) :154 ## Open_Porch_SF TotRms_AbvGrd First_Flr_SF Second_Flr_SF ## Min. : 0.00 Min. : 2.000 Min. : 334 Min. : 0.0 ## 1st Qu.: 0.00 1st Qu.: 5.000 1st Qu.: 877 1st Qu.: 0.0 ## Median : 27.00 Median : 6.000 Median :1092 Median : 0.0 ## Mean : 49.93 Mean : 6.413 Mean :1171 Mean : 319.6 ## 3rd Qu.: 72.00 3rd Qu.: 7.000 3rd Qu.:1426 3rd Qu.: 682.0 ## Max. :742.00 Max. :12.000 Max. :4692 Max. :2065.0 ## ## Year_Built ## Min. :1875 ## 1st Qu.:1954 ## Median :1972 ## Mean :1971 ## 3rd Qu.:2000 ## Max. :2010 ## nearZeroVar(ames, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## Sale_Price 1.000000 55.7321226 FALSE FALSE ## Gr_Liv_Area 1.333333 62.9965948 FALSE FALSE ## Garage_Type 2.196581 0.7945516 FALSE FALSE ## Garage_Cars 1.970213 0.5675369 FALSE FALSE ## Garage_Area 2.250000 38.0249716 FALSE FALSE ## Street 219.250000 0.2270148 FALSE TRUE ## Utilities 880.000000 0.2270148 FALSE TRUE ## Pool_Area 876.000000 0.6810443 FALSE TRUE ## Neighborhood 1.476744 2.9511918 FALSE FALSE ## Screen_Porch 199.750000 6.6969353 FALSE TRUE ## Overall_Qual 1.119816 1.1350738 FALSE FALSE ## Lot_Area 1.071429 79.7956867 FALSE FALSE ## Lot_Frontage 1.617021 11.5777526 FALSE FALSE ## MS_SubClass 1.959064 1.7026107 FALSE FALSE ## Misc_Val 141.833333 1.9296254 FALSE TRUE ## Open_Porch_SF 23.176471 19.2962543 FALSE FALSE ## TotRms_AbvGrd 1.311224 1.2485812 FALSE FALSE ## First_Flr_SF 1.777778 63.7911464 FALSE FALSE ## Second_Flr_SF 64.250000 31.3280363 FALSE FALSE ## Year_Built 1.116279 12.0317821 FALSE FALSE levels(ames$Overall_Qual) ## [1] &quot;Above_Average&quot; &quot;Average&quot; &quot;Below_Average&quot; &quot;Excellent&quot; ## [5] &quot;Fair&quot; &quot;Good&quot; &quot;Poor&quot; &quot;Very_Excellent&quot; ## [9] &quot;Very_Good&quot; &quot;Very_Poor&quot; # relevel the levels ames$Overall_Qual &lt;- factor(ames$Overall_Qual, levels = c(&quot;Very_Poor&quot;, &quot;Poor&quot;, &quot;Fair&quot;, &quot;Below_Average&quot;, &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Good&quot;, &quot;Very_Good&quot;, &quot;Excellent&quot;, &quot;Very_Excellent&quot;)) levels(ames$Overall_Qual) # the levels are properly ordered ## [1] &quot;Very_Poor&quot; &quot;Poor&quot; &quot;Fair&quot; &quot;Below_Average&quot; ## [5] &quot;Average&quot; &quot;Above_Average&quot; &quot;Good&quot; &quot;Very_Good&quot; ## [9] &quot;Excellent&quot; &quot;Very_Excellent&quot; Finding that there are 19 features in total and 13 of them are numerical variables, 6 of them are categorical variables. For the ordinal categorical variables Overall_Qual, finding that the level is not in order, hence, we releveled the level for this variable. Further, there’re 113 NA values and 5 nzv features in the whole dataset. Those are things we need to deal within blueprint. 2.1.2 Data Spliting set.seed(013123) index &lt;- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE) # consider 80-20 split ames_train &lt;- ames[index,] # training data ames_test &lt;- ames[-index,] # test data # investigate nominal categorical predictors ames_train %&gt;% count(Neighborhood) %&gt;% arrange(n) # check frequency of categories ## # A tibble: 26 × 2 ## Neighborhood n ## &lt;fct&gt; &lt;int&gt; ## 1 Northpark_Villa 2 ## 2 Blueste 2 ## 3 Greens 4 ## 4 Veenker 6 ## 5 Bloomington_Heights 7 ## 6 Briardale 8 ## 7 Clear_Creek 11 ## 8 Meadow_Village 12 ## 9 Stone_Brook 13 ## 10 Timberland 14 ## # … with 16 more rows "],["recipe-and-blueprint.html", "2.2 Recipe and Blueprint", " 2.2 Recipe and Blueprint After all preprocessing steps have been decided set up the overall blueprint. # set up recipe ames_recipe &lt;- recipe(Sale_Price~., data = ames_train) # specify feature engineering steps ames_blueprint &lt;- ames_recipe %&gt;% step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_integer(Overall_Qual) %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% step_scale(all_numeric(), -all_outcomes()) %&gt;% step_other(Neighborhood, threshold = 0.01, other = &quot;other&quot;) %&gt;% step_dummy(all_nominal(), one_hot = FALSE) Within the blueprint, we first filter out the zv/nzv predictors, and impute the missing entries. Then, we do numeric conversion of level of ordinal categorical variable Overall_Qual. Following with the data standardization step and lumping step. Finally, do the one-hot/dummy encode of nominal categorical variables. # estimate feature engineering parameters based on training data ames_prepare &lt;- prep(ames_blueprint, data = ames_train) # apply the blueprint to training data for building final/optimal model ames_baked_train &lt;- bake(ames_prepare, new_data = ames_train) # apply the blueprint to test data for future use ames_baked_test &lt;- bake(ames_prepare, new_data = ames_test) "],["analysis.html", "2.3 Analysis", " 2.3 Analysis 2.3.1 Bulid Model set.seed(111) # implement 5-fold CV repeated 5 times cv_specs &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats=5) # set tunegrid tunegrid &lt;- data.frame(method = &quot;GCV.Cp&quot;, select = TRUE) # GAM model ames_model&lt;-train(ames_blueprint, data = ames_train, method = &quot;gam&quot;, tuneGrid = tunegrid, trControl = cv_specs) We implement 5 fold CV with 5 repeats here to ensure the model is not overfitting and is generalizable to new data, which is very important for GAM model. For the model, Method = \"gam\" indicates the smoothing parameter estimation method, GCV for models with unknown scale parameter and Mallows’ Cp/UBRE/AIC for models with known scale; select=TRUE adds extra penalty so that the smoothing parameter estimation can completely remove terms from the model 2.3.2 Model’s Performance ames_model$results$RMSE ## [1] 70421.17 ames_model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + ## Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + ## Neighborhood_Somerset + Neighborhood_Northridge_Heights + ## Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + ## MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + ## MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + ## Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + ## s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + ## s(First_Flr_SF) + s(Lot_Area) ## ## Estimated degrees of freedom: ## 0.961 0.000 3.936 1.065 1.974 3.105 6.489 ## 8.662 7.680 total = 50.87 ## ## GCV score: 722968462 Here we could see that the RMSE is 70421.17, which is pretty high, probably due to the size of the dataset being big. The estimated degrees of freedom indicate the complexity of the model, and in this case, the EDF ranges from 0 to 8.662, with a total EDF of 50.87. The EDF values for the smoothing functions indicate the effective number of parameters used to fit the data, and higher EDF values generally indicate more flexible, complex models. The GCV score is used as a measure of the model’s performance because it provides a balance between the model’s fit to the data and its complexity. We could also look at the formula of the final model generated from training. It shows that out of the 53 variables, only 9 of the variables have a non-parametric relationship with the response. This could also be the reason for the high RMSE. "],["final-model.html", "2.4 Final Model", " 2.4 Final Model 2.4.1 Prediction on Test Dataset # build final model ames_final_model &lt;- gam(Sale_Price~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + s(First_Flr_SF) + s(Lot_Area),data=ames_baked_train) ames_final_model_preds&lt;- predict(object = ames_final_model, newdata = ames_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((ames_final_model_preds- ames_baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 195576.6 Using the formula provided from training, we tried to fit the GAM model. The RMSE of the final model is 195576.6, compared to 70421.17 previously. Obviously, this is not ideal, since now the RMSE is more than two times higher. Why? plot(ames_final_model, residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;,pch = 1, cex = 0.5,pages = 1) Here we have the residual plot of those variables that had applied splines function, and the blue shades represent the 95% confidence interval. From the graphs for Gr_Liv_Area and Lot_Area, we could see that the tails are drawn to the noises, letting it change the slope, which should not be happening. In order to create a well-fit final model, we have to recognize what is wrong with out current model. First, let’s look at the several plots gam.check had produced. set.seed(1293) gam.check(ames_final_model) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 26 iterations. ## The RMS GCV score gradient at convergence was 17.03165 . ## The Hessian was positive definite. ## Model rank = 98 / 98 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(TotRms_AbvGrd) 9.00 1.00 1.05 0.865 ## s(Lot_Frontage) 9.00 1.00 0.97 0.200 ## s(Year_Built) 9.00 3.94 0.92 0.015 * ## s(Open_Porch_SF) 9.00 1.00 0.91 &lt;2e-16 *** ## s(Second_Flr_SF) 9.00 1.00 0.95 0.085 . ## s(Garage_Area) 9.00 3.62 0.98 0.295 ## s(Gr_Liv_Area) 9.00 9.00 1.00 0.560 ## s(First_Flr_SF) 9.00 6.48 1.09 0.970 ## s(Lot_Area) 9.00 7.81 1.05 0.930 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the Q-Q plot, we could see that there is a clear trend to the residuals; from the residual values plot, we could see that there’s a cluster of points, and maybe a negative slope; from the histogram, we could see that there is a bell-shaped curve; and from the response vs. fitted values plot, we could see that the values are fitted pretty good. We should also look at the table generated from gam.check. The table shows the k value, effective degrees of freedom, test statistics, and p-value of each basis functions. The k value is very similar to the “knots” in MARS, representing how many basis function is used to fit the model, and could be used as the smoothing parameter. Another smoothing parameter we would use is λ, since \\(FIT=Likelihood-λ*Wiggliness\\) is how the fit of GAM model is calculated. The likelihood represents how well the model captures patterns in the data, and wiggliness represents the complexity of the model. As described in the model, when the p-value is too low, or if the k-index is below 1, it’s possible that we need to increase the size of the basis function. In the table above, we could see that Year_Built, Open_porch_SF, and Second_Flr_SF all had p-values below 0.1. # build final model ames_final_model2 &lt;- gam(Sale_Price~Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + s(Garage_Cars,sp=0.1,k=2) + s(Overall_Qual,k=3) + s(TotRms_AbvGrd,sp=0.01,k=11) + s(Lot_Frontage,sp=0.1,k=7) +s(Year_Built,sp=0.0001,k=29) + s(Open_Porch_SF,sp=0.6,k=9) + s(Second_Flr_SF,sp=0.9,k=7) + s(Garage_Area,sp=1,k=5) + s(Gr_Liv_Area,sp=10,k=6) + s(First_Flr_SF,sp=0.001,k=3) + s(Lot_Area,sp=1,k=3) ,data=ames_baked_train) After seeing where the problem was, we started using the smoothing parameter to tune the variables. From the formula \\(FIT=Likelihood-λ*Wiggliness\\), we know that, the more wiggly (complex) we want the variable to fit, the smaller λ needs to be (between 0 and 1), and if we want the fit to be less wiggly, λ has to be bigger than 1. plot(ames_final_model,select=c(7), residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;) For example, above is the plot from the first model of the variable Gr_Liv_Area. We can see that, when the above ground living area was increasing, the trend of the price goes up when there are more data, and when there is less data, the price started dramatically. This disobeys the common sense, since the bigger the house is, the more expensive the house should be. # s(Gr_Liv_Area,sp=10,k=6) plot(ames_final_model2,select=c(9), residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;) This is the reason why we need to use λ and tune the variable. For the second model, we chose a λ of 10, which means that we don’t want the trend to be wiggly, and it should be as simple as possible, which also prevents the line of best fit to be drawn by the noises. As we can see from the graph above, the price is now predicted to increase with the above ground area. set.seed(12984) gam.check(ames_final_model2) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 8 iterations. ## The RMS GCV score gradient at convergence was 3093.98 . ## The Hessian was positive definite. ## Model rank = 90 / 90 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(Garage_Cars) 2.00 1.81 1.03 0.77 ## s(Overall_Qual) 2.00 1.95 1.04 0.91 ## s(TotRms_AbvGrd) 10.00 6.24 0.99 0.30 ## s(Lot_Frontage) 6.00 3.43 0.98 0.28 ## s(Year_Built) 28.00 25.78 0.95 0.12 ## s(Open_Porch_SF) 8.00 2.53 0.97 0.18 ## s(Second_Flr_SF) 6.00 2.46 0.98 0.23 ## s(Garage_Area) 4.00 1.80 1.00 0.48 ## s(Gr_Liv_Area) 5.00 1.08 0.97 0.20 ## s(First_Flr_SF) 2.00 1.97 1.00 0.47 ## s(Lot_Area) 2.00 1.03 1.02 0.74 After tuning, we can see that now the basis dimension for all variables have p-values of bigger than 0.1. ames_final_model_preds2&lt;- predict(object = ames_final_model2, newdata = ames_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((ames_final_model_preds2- ames_baked_test$Sale_Price)^2)) # calculate test set RMSE ## [1] 28781.85 Here, we can also see that the RMSE value had decreased to 28781.85, compared to 195576.6 from the first model. "],["boston-housing-dataset-example.html", "3 Boston housing dataset Example", " 3 Boston housing dataset Example Data description: Orignially, since GAM did not work well with the Ames housing dataset, which output a big RMSE, we decided to use another dataset and to test the GAM method on it again. The new dataset, Boston housing, includes 13 explanatory variables, and the response variable MEDV. The following is the description from Kaggle all variables: Response Variable: MEDV: Median value of owner-occupied homes in $1000’s [k$] Explanatory variables: CRIM: per capita crime rate by town ZN: proportion of residential land zoned for lots over 25,000 sq.ft. INDUS: proportion of non-retail business acres per town CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) NOX: nitric oxides concentration (parts per 10 million) [parts/10M] RM: average number of rooms per dwelling AGE: proportion of owner-occupied units built prior to 1940 DIS: weighted distances to five Boston employment centres RAD: index of accessibility to radial highways TAX: full-value property-tax rate per $10,000 [$/10k] PTRATIO: pupil-teacher ratio by town B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town LSTAT: % lower status of the population "],["preparation-steps-1.html", "3.1 Preparation Steps", " 3.1 Preparation Steps 3.1.1 Data Investigation Just as before, we first investigate the dataset to see if there’s any missing entries, non-numeric variables, or ZV/NZV features. glimpse(boston) ## Rows: 506 ## Columns: 14 ## $ CRIM &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ ZN &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ INDUS &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ CHAS &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ NOX &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ RM &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ AGE &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ DIS &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ RAD &lt;dbl&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ TAX &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ PTRATIO &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ B &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ LSTAT &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ MEDV &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… summary(boston) ## CRIM ZN INDUS CHAS ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## NOX RM AGE DIS ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## RAD TAX PTRATIO B ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## LSTAT MEDV ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00 sum(is.na(boston)) ## [1] 0 nearZeroVar(boston) ## integer(0) Finding that there are 13 features in total and all of them are numerical variables. There is no NA value in the whole dataset and no zv or nzv features. 3.1.2 Data Spliting set.seed(013123) index &lt;- createDataPartition(y = boston$MEDV, p = 0.8, list = FALSE) # consider 80-20 split #Data Spliting boston_train &lt;- boston[index,] # training data boston_test &lt;- boston[-index,] # test data "],["recipe-and-blueprint-1.html", "3.2 Recipe and Blueprint", " 3.2 Recipe and Blueprint # Set up recipe boston_recipe &lt;- recipe(MEDV ~ ., data = boston_train) # set up blueprint boston_blueprint &lt;- boston_recipe %&gt;% step_center(all_numeric(), -all_outcomes()) %&gt;% # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response) step_scale(all_numeric(), -all_outcomes()) # scale (divide by standard deviation) all numeric predictors Since there’s no missing value, no zv/nzv features and no categorical variable in boston dataset, all we need in blueprint is to standardize the data. # estimate feature engineering parameters based on training data boston_prepare &lt;- prep(boston_blueprint, data = boston_train) # apply the blueprint to training data for building final/optimal model boston_baked_train &lt;- bake(boston_prepare, new_data = boston_train) # apply the blueprint to test data for future use boston_baked_test &lt;- bake(boston_prepare, new_data = boston_test) "],["analysis-1.html", "3.3 Analysis", " 3.3 Analysis 3.3.1 Bulid Model set.seed(111) # implement 5-fold CV with no repeat cv_specs &lt;- trainControl(method = &quot;cv&quot;, number = 5) # set tunegrid tunegrid &lt;- data.frame(method = &quot;GCV.Cp&quot;, select = TRUE) # GAM model boston_model &lt;- train(boston_blueprint, data = boston_train, method = &quot;gam&quot;, tuneGrid = data.frame(method = &quot;GCV.Cp&quot;, select = TRUE), trControl = cv_specs) We implement CV with no repeat here to ensure the model is not overfitting and is generalizable to new data, which is very important for GAM model. 3.3.2 Model’s Performance # RMSE boston_model$results$RMSE ## [1] 3.989444 boston_model$finalMo ## ## Family: gaussian ## Link function: identity ## ## Formula: ## .outcome ~ CHAS + RAD + s(ZN) + s(PTRATIO) + s(TAX) + s(INDUS) + ## s(NOX) + s(B) + s(AGE) + s(DIS) + s(RM) + s(LSTAT) + s(CRIM) ## ## Estimated degrees of freedom: ## 0.000 0.927 2.906 5.636 7.931 4.886 0.000 ## 0.889 8.392 5.813 4.421 total = 44.8 ## ## GCV score: 12.95974 From the result, seeing that the RMSE is very small here, around 4, which indicates GAM model works well with train boston dataset. "],["final-model-1.html", "3.4 Final Model", " 3.4 Final Model 3.4.1 Prediction on Test Dataset # obtain predictions and test set RMSE boston_final_model &lt;- gam(MEDV ~ CHAS + RAD + s(ZN, sp = 1, k = 8) + s(PTRATIO, sp = 0.1, k = 12) + s(TAX, sp = 1.2, k = 13) + s(INDUS,sp = 0.1, k = 12) + s(NOX, sp = 0.01, k = 12) + s(B, sp = 8, k = 9) + s(AGE, k = 9) + s(DIS,sp = 0.01, k = 9) + s(RM,sp = 0.8, k = 9) + s(LSTAT, k = 9) + s(CRIM, k = 9), data = boston_baked_train) boston_final_model_preds&lt;- predict(object = boston_final_model, newdata = boston_baked_test, type = &quot;response&quot;) # obtain predictions sqrt(mean((boston_final_model_preds - boston_baked_test$MEDV)^2)) # calculate test set RMSE ## [1] 2.628545 The test set RMSE is even smaller, which is a good sign. Comparing with the first model using GAM model for ames dataset, using it for boston dataset seems to be a lot better. The reason to that is the fact that the boston dataset is a lot more complex then the ames dataset. plot(boston_final_model, residuals=TRUE,shade = TRUE, shade.col = &quot;lightblue&quot;, pch = 1) Let’s look at the residual plot for the boston final model. As we can see from above, only two of the graphs seem to have a slope close to 0, and lots of others with non-parametric relationships. This is a sign that the model is way more complex, compared to the ames dataset, where only 3 explanatory variables seemed to have non-parametric relationship with the response. set.seed(111) gam.check(boston_final_model) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 14 iterations. ## The RMS GCV score gradient at convergence was 8.826314e-07 . ## The Hessian was positive definite. ## Model rank = 103 / 103 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(ZN) 7.00 2.28 0.71 &lt;2e-16 *** ## s(PTRATIO) 11.00 5.12 0.72 &lt;2e-16 *** ## s(TAX) 12.00 3.57 0.72 &lt;2e-16 *** ## s(INDUS) 11.00 4.30 0.68 &lt;2e-16 *** ## s(NOX) 11.00 8.39 0.88 &lt;2e-16 *** ## s(B) 8.00 1.42 0.99 0.45 ## s(AGE) 8.00 1.00 1.05 0.80 ## s(DIS) 8.00 6.42 0.84 &lt;2e-16 *** ## s(RM) 8.00 3.16 0.93 0.04 * ## s(LSTAT) 8.00 6.93 1.03 0.76 ## s(CRIM) 8.00 4.42 1.03 0.77 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 However, after several attempts, we were not able to increase the p-value of the basis dimension values and k-index. "],["conclusion.html", "4 Conclusion", " 4 Conclusion Even though the RMSE from the boston final model was better than that of ames, we were still not satisfied with the result. The first problem is the low p-value for the basis k dimension from the boston final model. In the process of trying to increase the p-value, we had came across the possible explanation, that the concurvity of our model is too high. concurvity(boston_final_model,full=TRUE) ## para s(ZN) s(PTRATIO) s(TAX) s(INDUS) s(NOX) ## worst 3.594799e-17 0.9287231 0.9693997 0.9998736 0.9998697 0.9668546 ## observed 3.594799e-17 0.7498905 0.8759169 0.9332306 0.9215548 0.6775295 ## estimate 3.594799e-17 0.8690648 0.8379190 0.9600096 0.9111579 0.8281045 ## s(B) s(AGE) s(DIS) s(RM) s(LSTAT) s(CRIM) ## worst 0.5103302 0.8551451 0.9495880 0.7725760 0.8605279 0.9823724 ## observed 0.4525747 0.8311019 0.8860317 0.6858639 0.8224553 0.7821311 ## estimate 0.4503063 0.7763011 0.8481623 0.5836800 0.7448919 0.3287977 From the concurvity table, which shows how much each smooth is predetermined by all the other smooths,notice that for all variables except for B, all of the smooths has at least 0.8 of worst case concurvity. The problem with having concurvity is that, if two variables have high concurvity and form a perfect parabola with each other, the confidence interval would be really wide, and negatively affect the ability of model prediction. concurvity(ames_final_model2,full=TRUE) ## para s(Garage_Cars) s(Overall_Qual) s(TotRms_AbvGrd) ## worst 0.8515993 0.9225410 0.7815817 0.7880980 ## observed 0.8515993 0.7601708 0.7791756 0.5239233 ## estimate 0.8515993 0.8729244 0.7568255 0.7343907 ## s(Lot_Frontage) s(Year_Built) s(Open_Porch_SF) s(Second_Flr_SF) ## worst 0.5672953 0.8254358 0.4569746 0.9287979 ## observed 0.5442263 0.6477651 0.2792555 0.9049044 ## estimate 0.3707956 0.6689463 0.3191145 0.8286263 ## s(Garage_Area) s(Gr_Liv_Area) s(First_Flr_SF) s(Lot_Area) ## worst 0.9528111 0.9431845 0.9286778 0.6092605 ## observed 0.6167421 0.9185987 0.9109896 0.5703752 ## estimate 0.7947228 0.8660314 0.9233605 0.5703277 Here we can see the concurvity table of our second ames model. Similarly, this model also has some high worst case concurvities, but slightly less in nunmber compared to the boston model. From both the Q-Q plots, we can see the problem with concurvity more clearly. Both models still have trends in the graph, but ames model performs a little better by being more linear. In conclusion, when using Generalized Additive models, it’s a good idea to always check the concurvity within the models, even if you’re getting very low RMSE values. It’s possible that through using smoothing parameters, the RMSE lowers, but the existence of high concurvity would always be a bad sign in using GAM. "]]
