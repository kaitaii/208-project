
# Boston housing dataset Example

Data description:

Since GAM did not work well with the Ames housing dataset, which output a big RMSE, we decided to use another dataset and to test the GAM method on it again. The new dataset, Boston housing, includes 13 explanatory variables, and the response variable `MEDV`. The following is the description from Kaggle all variables:

Response Variable: 

1) `MEDV`: Median value of owner-occupied homes in `$`1000's [k`$`]


Explanatory variables:

1) `CRIM`: per capita crime rate by town

2) `ZN`: proportion of residential land zoned for lots over 25,000 sq.ft.

3) `INDUS`: proportion of non-retail business acres per town

4) `CHAS`: Charles River dummy variable (1 if tract bounds river; 0 otherwise)

5) `NOX`: nitric oxides concentration (parts per 10 million) [parts/10M]

6) `RM`: average number of rooms per dwelling

7) `AGE`: proportion of owner-occupied units built prior to 1940

8) `DIS`: weighted distances to five Boston employment centres

9) `RAD`: index of accessibility to radial highways

10) `TAX`: full-value property-tax rate per `$`10,000 [`$`/10k]

11) `PTRATIO`: pupil-teacher ratio by town

12) `B`: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town

13) `LSTAT`: % lower status of the population


```{r,echo=FALSE,warning=FALSE, include=FALSE}
# load package and dataset

library(tidyverse)
library(caret)
library(recipes)
library(mgcv)

boston<-read_csv("boston.csv") # load package and dataset
```

## Preparation Steps

### Data Investigation

Just as before, we first investigate the dataset to see if there's any missing entries, non-numeric variables, or ZV/NZV features.

```{r}
glimpse(boston)  
summary(boston) 
sum(is.na(boston)) 
nearZeroVar(boston) 
```

Finding that there are 13 features in total and all of them are numerical variables. There is no NA value in the whole dataset and no zv or nzv features.

```{r}
cor(boston)
```

From the correlation plot, noticing that for variable `TAX` and `INDUS`, `DIS` and `INDUS`, `TAX` and `INDUS`, `NOX` and `INDUS`, `NOX` and `AGE`, `NOX` and `DIS`, `AGE` and `DIS`... are highly correlated variable. (We are saying two variables are highly correlated when their correlation value is greater that 0.7)


### Data Spliting

```{r}
set.seed(013123)  

index <- createDataPartition(y = boston$MEDV, p = 0.8, list = FALSE)   # consider 80-20 split

#Data Spliting 

boston_train <- boston[index,]   # training data

boston_test <- boston[-index,]   # test data
```

## Recipt and Blueprint

```{r}
# Set up recipe 
boston_recipe <- recipe(MEDV ~ ., data = boston_train)

# set up blueprint
boston_blueprint <- boston_recipe %>%    
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response)
  step_scale(all_numeric(), -all_outcomes())                     # scale (divide by standard deviation) all numeric predictors                     
```

Since there's no missing value, no zv/nzv features and no categorical variable in `boston` dataset, all we need in blueprint is to standardize the data. 

```{r}
# estimate feature engineering parameters based on training data
boston_prepare <- prep(boston_blueprint, data = boston_train)   

# apply the blueprint to training data for building final/optimal model
boston_baked_train <- bake(boston_prepare, new_data = boston_train)   

# apply the blueprint to test data for future use
boston_baked_test <- bake(boston_prepare, new_data = boston_test)    
```

## Analysis

### Bulid Model

```{r}
set.seed(111)
# implement 5-fold CV with no repeat
cv_specs <- trainControl(method = "cv", number = 5)

# set tunegrid
tunegrid <- data.frame(method = "GCV.Cp", select = TRUE)

# GAM model
boston_model <- train(boston_blueprint,
                      data = boston_train,
                      method = "gam",
                      tuneGrid = data.frame(method = "GCV.Cp", select = TRUE),
                      trControl = cv_specs)
```

We implement CV with no repeat here to ensure the model is not overfitting and is generalizable to new data, which is very important for GAM model.

### Model's Performance

```{r}
# RMSE
boston_model$results$RMSE

boston_model$finalMo
```

From the result, seeing that the RMSE is very small here, around 4, which indicates GAM model works well with train `boston` dataset. 

The estimated degrees of freedom indicate the complexity of the model, and in this case, the EDF ranges from 0 to 8.392, with a total EDF of 44.8. The EDF values for the smoothing functions indicate the effective number of parameters used to fit the data, and higher EDF values generally indicate more flexible, complex models.

The GCV score is used as a measure of the model's performance because it provides a balance between the model's fit to the data and its complexity. 

## Final Model

### Prediction on Test Dataset

```{r}
# obtain predictions and test set RMSE

boston_final_model <- gam(MEDV ~ CHAS + RAD + s(ZN, sp = 1, k = 8) + s(PTRATIO, sp = 0.1, k = 12) + s(TAX, sp = 1.2, k = 13) + s(INDUS,sp = 0.1, k = 12) + s(NOX, sp = 0.01, k = 12) + s(B, sp = 8, k = 9) + s(AGE, k = 9) + s(DIS,sp = 0.01, k = 9) + s(RM,sp = 0.8, k = 9) + s(LSTAT, k = 9) + s(CRIM, k = 9), data = boston_baked_train) 

boston_final_model_preds<- predict(object = boston_final_model, newdata = boston_baked_test, type = "response")    # obtain predictions

sqrt(mean((boston_final_model_preds - boston_baked_test$MEDV)^2))   # calculate test set RMSE
```

The test set RMSE is even smaller, which is a good sign. 

Comparing with using GAM model for `ames` dataset, using it for `boston` dataset seems to be a lot better. 
The reason to that is the fact that the `boston` dataset is a lot more complex then the `ames` dataset. 

```{r}
boston_final_model$sp
```

```{r}
plot(boston_final_model, residuals=TRUE,shade = TRUE, shade.col = "lightblue", pch = 1)
```

Let's look at the residual plot for the `boston` final model. 
As we can see from above, only two of the graphs seem to have a slope close to 0, and lots of others with non-parametric relationships. 
This is a sign that the model is way more complex, compared to the `ames` dataset, where only 3 explanatory variables seemed to have non-parametric relationship with the response.


```{r}
set.seed(111)
gam.check(boston_final_model)
```



