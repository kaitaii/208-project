---
title: "CMSC/LING/STAT 208: Final Project - GAMs"
author: "Yi Lu, Shiyi Yang"
output: html_document
---

# Introducation

The topic we wish to explore is the Generalized Additive Models. It is a powerful extension of the linear regression model that can capture complex nonlinear relationships between predictors and response variables, which allows for non-linear function of each variable, while maintaining their additivity. So, instead of $y = \beta_0 + \beta_1x_1 + \beta_2x_2+...+\epsilon$, we would have a function of $y = \beta_0 + f_1(x_1) + f_2(x_2) + ... +\epsilon$, where $f(xi)$ represent a (smooth) non-linear function. They can also model interactions between variables and handle different types of data, including continuous, categorical, and ordinal variables. 


# Example

## Ames Housing Dataset Example

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(recipes)
library(mgcv)
ames<-readRDS("AmesHousing.rds")
```

```{r}
glimpse(ames)  # Knowing that for this dataset n = 881, p = 20-1 = 19.
sum(is.na(ames))    # check for missing entries
summary(ames)  # check types of features, which features have missing entries?
levels(ames$Overall_Qual)   # the levels are NOT properly ordered

# relevel the levels

ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                  "Average", "Above_Average", "Good", "Very_Good", 
                                                  "Excellent", "Very_Excellent"))

levels(ames$Overall_Qual)   # the levels are properly ordered


```

```{r}
# split the dataset

set.seed(013123)   # set seed

index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE)   # consider 70-30 split

ames_train <- ames[index,]   # training data

ames_test <- ames[-index,]   # test data
```


```{r}
# investigate nominal categorical predictors 

ames_train %>% count(Neighborhood) %>% arrange(n)   # check frequency of categories
```

```{r}
# finally, after all preprocessing steps have been decided set up the overall blueprint

ames_recipe <- recipe(Sale_Price~., data = ames_train)   # set up recipe

# specify feature engineering steps
blueprint <- ames_recipe %>%    
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors   
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response)
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                            # one-hot/dummy encode nominal categorical predictors

# replace step_center and step_scale with step_normalize

prepare <- prep(blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = ames_train)   # apply the blueprint to training data for building final/optimal model

baked_test <- bake(prepare, new_data = ames_test)    # apply the blueprint to test data for future use
```

```{r}
set.seed(111)
model<-train(blueprint,
             data=ames_train,
             method="gam",
             tuneGrid=data.frame(method = "GCV.Cp", select = TRUE),  
             trControl=trainControl(method="cv",number=5)
)
model$results$RMSE

model$finalMo
```


```{r}
# obtain predictions and test set RMSE

final_model <- gam(Sale_Price ~ Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + s(First_Flr_SF) + s(Lot_Area) ,data=baked_train) 

final_model_preds<- predict(object = final_model, newdata = baked_test, type = "response")    # obtain predictions

sqrt(mean((final_model_preds - baked_test$Sale_Price)^2))   # calculate test set RMSE
```

Noticing that the RMSE for both train dataset and test dataset are all large, especailly for test dataset, which means the GAM model didn't work well for ames housing dataset. In order to show a better performace, we found another dataset, boston housing, to have a try. 

## Boston Housing Dataset Example



# Conclusion













