---
title: "CMSC/LING/STAT 208: Final Project"
author: "Yi Lu, Shiyi Yang"
output: html_document
---

```{r setup, include=FALSE}
# LEAVE THIS CODE CHUNK AS-IS
# This code chunk sets the global codechunk options for this file/chapter
knitr::opts_chunk$set(error = TRUE, include=TRUE, echo=TRUE, message=TRUE, warning=TRUE)
# For BP, telling R to not stop on error (error=TRUE option above) and changing errors to display with yellow font on crimson background
knitr::knit_hooks$set(error = function(x, options) { 
  paste(c('\n\n:::{style="color:Yellow; background-color: Crimson;"}',
        gsub('^## Error', '**ERROR:**', x),
        ':::'), collapse = '\n')
})
```

```{r,echo=FALSE,warning=FALSE, include=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(mgcv)
boston<-read_csv("boston.csv")
```

```{r}
glimpse(boston)  # p=13
sum(is.na(boston))    # check for missing entries
summary(boston)  # check types of features, which features have missing entries?
nearZeroVar(boston)
```

```{r}
# split the dataset

set.seed(013123)   # set seed

index <- createDataPartition(y = boston$MEDV, p = 0.8, list = FALSE)   # consider 70-30 split

boston_train <- boston[index,]   # training data

boston_test <- boston[-index,]   # test data
```



```{r}
# finally, after all preprocessing steps have been decided set up the overall blueprint

boston_recipe <- recipe(MEDV~., data = boston_train)   # set up recipe

# specify feature engineering steps
blueprint <- boston_recipe %>%    
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response)
  step_scale(all_numeric(), -all_outcomes())                     # scale (divide by standard deviation) all numeric predictors                     

# replace step_center and step_scale with step_normalize

prepare <- prep(blueprint, data = boston_train)    # estimate feature engineering parameters based on training data


baked_train <- bake(prepare, new_data = boston_train)   # apply the blueprint to training data for building final/optimal model

baked_test <- bake(prepare, new_data = boston_test)    # apply the blueprint to test data for future use
```

```{r}
set.seed(111)
model<-train(blueprint,
             data=boston_train,
             method="gam",
             tuneGrid=data.frame(method = "GCV.Cp", select = TRUE), #method indicates the smoothing parameter estimation method, GCV for models with unknown scale parameter and Mallows' Cp/UBRE/AIC for models with known scale; select=TRUE adds extra penalty so that the smoothign parameter estimation can completely remove terms from the model 
             trControl=trainControl(method="cv",number=5)
)
model$results$RMSE

model$finalMo
```
```{r}
# obtain predictions and test set RMSE

#final_model <- gam(MEDV~.,data=baked_train) 

final_model_preds<- predict(object = model$finalModel, newdata = baked_test, type = "response")    # obtain predictions

sqrt(mean((final_model_preds - baked_test$MEDV)^2))   # calculate test set RMSE
```

```{r}
library(vip)
coef(model$finalModel)
```


















