
# Ames Housing Dataset Example

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(caret)
library(recipes)
library(mgcv)
ames<-readRDS("AmesHousing.rds")
```


```{r}
glimpse(ames)  # Knowing that for this dataset n = 881, p = 20-1 = 19.
sum(is.na(ames))    # check for missing entries
summary(ames)  # check types of features, which features have missing entries?
levels(ames$Overall_Qual)   # the levels are NOT properly ordered

# relevel the levels

ames$Overall_Qual <- factor(ames$Overall_Qual, levels = c("Very_Poor", "Poor", "Fair", "Below_Average", 
                                                  "Average", "Above_Average", "Good", "Very_Good", 
                                                  "Excellent", "Very_Excellent"))

levels(ames$Overall_Qual)   # the levels are properly ordered


```

```{r}
# split the dataset

set.seed(013123)   # set seed

index <- createDataPartition(y = ames$Sale_Price, p = 0.8, list = FALSE)   # consider 70-30 split

ames_train <- ames[index,]   # training data

ames_test <- ames[-index,]   # test data
```


```{r}
# investigate nominal categorical predictors 

ames_train %>% count(Neighborhood) %>% arrange(n)   # check frequency of categories
```

```{r}
# finally, after all preprocessing steps have been decided set up the overall blueprint

ames_recipe <- recipe(Sale_Price~., data = ames_train)   # set up recipe

# specify feature engineering steps
ames_blueprint <- ames_recipe %>%    
  step_nzv(Street, Utilities, Pool_Area, Screen_Porch, Misc_Val) %>%   # filter out zv/nzv predictors
  step_impute_mean(Gr_Liv_Area) %>%                                    # impute missing entries
  step_integer(Overall_Qual) %>%                                       # numeric conversion of levels of the predictors   
  step_center(all_numeric(), -all_outcomes()) %>%                      # center (subtract mean) all numeric predictors (consider all the numeric predictors except the response)
  step_scale(all_numeric(), -all_outcomes()) %>%                       # scale (divide by standard deviation) all numeric predictors
  step_other(Neighborhood, threshold = 0.01, other = "other") %>%      # lumping required predictors
  step_dummy(all_nominal(), one_hot = FALSE)                            # one-hot/dummy encode nominal categorical predictors

# replace step_center and step_scale with step_normalize

ames_prepare <- prep(ames_blueprint, data = ames_train)    # estimate feature engineering parameters based on training data


ames_baked_train <- bake(ames_prepare, new_data = ames_train)   # apply the blueprint to training data for building final/optimal model

ames_baked_test <- bake(ames_prepare, new_data = ames_test)    # apply the blueprint to test data for future use
```




```{r}
set.seed(111)
ames_model<-train(ames_blueprint,
             data=ames_train,
             method="gam",
             tuneGrid=data.frame(method = "GCV.Cp", select = TRUE),  
             trControl=trainControl(method="cv",number=5)
)
ames_model$results$RMSE

ames_model$finalMo
```


```{r}
  
# build final model
ames_final_model <- gam(Sale_Price~Garage_Type_BuiltIn + Garage_Type_Detchd + Garage_Type_No_Garage + Neighborhood_College_Creek + Neighborhood_Old_Town + Neighborhood_Edwards + Neighborhood_Somerset + Neighborhood_Northridge_Heights + Neighborhood_Gilbert + Neighborhood_Sawyer + MS_SubClass_One_and_Half_Story_Finished_All_Ages + MS_SubClass_Two_Story_1946_and_Newer + MS_SubClass_Duplex_All_Styles_and_Ages + 
MS_SubClass_One_Story_PUD_1946_and_Newer + Garage_Cars + Overall_Qual + s(TotRms_AbvGrd) + s(Lot_Frontage) + s(Year_Built) + s(Open_Porch_SF) + s(Second_Flr_SF) + s(Garage_Area) + s(Gr_Liv_Area) + s(First_Flr_SF) + s(Lot_Area) ,data=ames_baked_train) 

# final model predication
ames_final_model_preds<- predict(object = ames_final_model, newdata = ames_baked_test, type = "response")  

sqrt(mean((ames_final_model_preds - ames_baked_test$Sale_Price)^2))   # calculate test set RMSE
```




```{r}
plot(ames_final_model, residuals=TRUE,shade = TRUE, shade.col = "lightblue")
```


Noticing that the RMSE for both train dataset and test dataset are all large, especailly for test dataset, which means the GAM model didn't work well for ames housing dataset. In order to show a better performace, we found another dataset, boston housing, to have a try. 

```{r}
ames_final_model2 <- gam(Sale_Price~ s(Gr_Liv_Area) + 
    s(First_Flr_SF) + s(Lot_Area) ,data=ames_baked_train) 
ames_final_model_preds2<- predict(object = ames_final_model2, newdata = ames_baked_test, type = "response")    # obtain predictions

sqrt(mean((ames_final_model_preds2 - ames_baked_test$Sale_Price)^2))   # calculate test set RMSE
```


## Boston Housing Dataset Example



```{r}
ggplot(ames,aes(x=Gr_Liv_Area,y=Sale_Price))+geom_point()+geom_smooth()
```

```{r}
gam_mod <- gam(Sale_Price ~ s(Gr_Liv_Area), data = ames)
plot(gam_mod)
```





```{r}
ames_final_model2 <- gam(Sale_Price~ s(Gr_Liv_Area) + 
    s(First_Flr_SF) + s(Lot_Area) ,data=ames_baked_train) 
ames_final_model_preds2<- predict(object = ames_final_model2, newdata = ames_baked_test, type = "response")    # obtain predictions

sqrt(mean((ames_final_model_preds2 - ames_baked_test$Sale_Price)^2))   # calculate test set RMSE
```











